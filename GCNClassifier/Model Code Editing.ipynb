{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     12,
     21,
     29,
     73,
     86
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting layers.py\n"
     ]
    }
   ],
   "source": [
    "%%file layers.py\n",
    "# %load layers.py\n",
    "from inits import *\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res    \n",
    "    \n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens a tensor layer.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Flatten, self).__init__(**kwargs)\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        # flatten the tensor to one layer\n",
    "        shape = x.get_shape().as_list()               # a list: [None,...]\n",
    "        dim = np.prod(shape[1:])                   # dim = prod(...)\n",
    "        x_flattened = tf.reshape(x, [-1, dim])        # -1 means \"all\"\n",
    "        return x_flattened\n",
    "    \n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "        # add dummy dimension\n",
    "        x = tf.expand_dims(x, 1) # Batch1M\n",
    "        \n",
    "        # transform\n",
    "        (batch, n, m) = x.get_shape().as_list()\n",
    "        p = self.vars['weights'].get_shape().as_list()[1]\n",
    "        output = tf.reshape(tf.reshape(x, [-1, m]) @ self.vars['weights'], [-1, n, p]) # BatchNM * MP => Batch1P (N should be 1)\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output) # Batch1P\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False,\n",
    "                **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.bias = bias\n",
    "        self.num_nodes = self.support.get_shape().as_list()[2]\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            # make all weight matrices for supports in convolution\n",
    "            for i in range(self.support.get_shape().as_list()[1]):# support: ?xSupportsxNxNxM\n",
    "                for j in range(self.support.get_shape().as_list()[4]):\n",
    "                    tensor_name = 'weights_support_' + str(i) + '_M_' + str(j)\n",
    "                    self.vars[tensor_name] = glorot([input_dim, output_dim], name=tensor_name)\n",
    "            # make vector to do weighted sum of all convolved features (w in SUM(wi*(NxF')) for w in M)\n",
    "            self.vars[\"Features Combination\"] = uniform(self.support.get_shape().as_list()[4],\n",
    "                                                            name=\"Features Combination\")\n",
    "            # make bias matrice\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "            \n",
    "        \n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # dropout\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "        # convolve\n",
    "        convolved_features = []\n",
    "        for j in range(self.support.get_shape().as_list()[4]):\n",
    "            temp = []\n",
    "            for i in range(self.support.get_shape().as_list()[1]):\n",
    "                tensor_name = 'weights_support_' + str(i) + '_M_' + str(j)\n",
    "                # BatchNF * FF' weight tensor (num_nodes, |Features'|)\n",
    "                (N, F) = x.get_shape().as_list()[1:]\n",
    "                embed = tf.reshape(x, [-1, F])\n",
    "                pre_sup =  tf.reshape(tf.reshape(x, [-1, F]) @ self.vars[tensor_name], [-1, N, self.output_dim])\n",
    "                (batch, _, F_new) = pre_sup.get_shape().as_list()\n",
    "\n",
    "                # BatchNN * BatchNF' => BatchNF'\n",
    "                support = tf.slice(self.support, [0,i,0,0,j], [-1,1,-1,-1,1]) # get Batch1NN1\n",
    "                support = tf.reshape(support, [-1,N,N]) # reshape to BatchNN\n",
    "                support = support @ pre_sup # now BatchNF'\n",
    "                temp.append(support)\n",
    "            # adds together list of BatchNF' into one BatchNF' for a single original adjacency matrix\n",
    "            convolved_F = tf.add_n(temp)\n",
    "            convolved_features.append(convolved_F)\n",
    "        # stack list into one tensor of shape BatchNF'M\n",
    "        convolved_features = tf.stack[convolved_features, axis = 3]\n",
    "        # do weighted multiplication\n",
    "        tf.multiply(convolved_features, self.vars[\"Features Combination\"])\n",
    "        # sum together to remove 4th dimension\n",
    "        output = tf.add_n(convolved_features, axis = 3)\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias'] # Broadcasting spreads bias across Batch and Node dimensions\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    \"\"\"Self attention layer, input is in ?xNxhidden, output is in ?x(Bias*Hidden). Hidden should correspond \n",
    "    to the number of features nodes have.\"\"\"\n",
    "    \n",
    "    def __init__(self, attention_dim, bias_dim, hidden_units, placeholders, dropout=0., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.A = None\n",
    "        self.vars['Ws'] = tf.Variable(tf.random_uniform([attention_dim, self.hidden_units])) # AttentionxHidden\n",
    "        self.vars['W2'] = tf.Variable(tf.random_uniform([bias_dim, attention_dim])) # BiasxAttention\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        \n",
    "        # dropout\n",
    "        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "        \n",
    "        # AttentionxHidden * ?xHiddenxN => ?xAttentionxN\n",
    "        inputsT = tf.transpose(inputs, perm = [0, 2, 1]) # transpose the inner matrices which is our intention\n",
    "        \n",
    "        #print(\"inputsT is of shape {}\".format(inputsT.get_shape().as_list()))\n",
    "        #print(\"self.vars[Ws] is of shape {}\".format(self.vars['Ws'].get_shape().as_list()))\n",
    "        \n",
    "        aux = tf.einsum('ah,bhn->ban', self.vars['Ws'], inputsT)\n",
    "        aux = tf.tanh(aux)\n",
    "        \n",
    "        #print(\"aux is of shape {}\".format(aux.get_shape().as_list()))\n",
    "        \n",
    "        \n",
    "        # BiasxAttention * ?xAttentionxN => ?xBiasxN\n",
    "        self.A = tf.einsum('ba,uan->ubn',self.vars['W2'], aux)\n",
    "        self.A = tf.nn.softmax(self.A)\n",
    "        #print(\"A is of shape {}\".format(self.A.get_shape().as_list()))\n",
    "        #tf.summary.histogram('self_attention', self.A) For visualization\n",
    "        \n",
    "        # ?xBiasxN * ?xNxHidden => ?xBiasxHidden\n",
    "        out = self.A @ inputs\n",
    "        #print(\"out is of shape {}\".format(out.get_shape().as_list()))\n",
    "        \n",
    "        # ?xBiasxHidden => ?x(Bias*Hidden)\n",
    "        out = tf.reshape(out, [ -1, out.get_shape().as_list()[1] * out.get_shape().as_list()[2]])\n",
    "        #print(\"out is of shape {}\".format(out.get_shape().as_list()))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     89
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models.py\n"
     ]
    }
   ],
   "source": [
    "%%file models.py\n",
    "# %load models.py\n",
    "from layers import *\n",
    "from metrics import *\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "        self.logits = None\n",
    "        self.predictions = None\n",
    "        \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        #print(\"\\n\\n\\nactivations\\n{}\\n\\n\\n\".format(self.activations))\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        \n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "        self.number_nodes = placeholders['features'].get_shape().as_list()[1]\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        \n",
    "        #print(\"placeholders and descriptions:\")\n",
    "        #for key in self.placeholders:\n",
    "        #    print(\"{}: {}\".format(key, self.placeholders[key]))\n",
    "        \n",
    "        #print(\"Model's input {}, output {}\".format(self.input_dim, self.output_dim))\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "        \n",
    "        # Cross entropy error\n",
    "        labels = self.placeholders['labels']\n",
    "        logits = self.outputs\n",
    "        dim = logits.get_shape().as_list()[2]\n",
    "        logits = tf.reshape(logits, [-1, dim])\n",
    "        entropies = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels, dim = -1)\n",
    "        loss = tf.reduce_mean(entropies)\n",
    "        #print(\"Labels {} \\nLogits {}\\nEntropies {}\\nLoss {}\\n\\n\".format(labels, logits, entropies, loss))\n",
    "        self.loss += loss\n",
    "\n",
    "    def _accuracy(self):\n",
    "        labels = self.placeholders['labels']\n",
    "        logits = self.outputs\n",
    "        dim = logits.get_shape().as_list()[2]\n",
    "        logits = tf.reshape(logits, [-1, dim])\n",
    "        self.logits = logits\n",
    "        labels=tf.argmax(labels, 1) # labels\n",
    "        predictions=tf.argmax(logits, 1) # prediction as one hot\n",
    "        self.predictions = predictions\n",
    "        \n",
    "        # Define the metric and update operations\n",
    "        tf_metric, tf_metric_update = tf.metrics.accuracy(predictions = predictions, labels = labels, name = \"accuracy\")\n",
    "        self.accuracy = tf_metric_update\n",
    "        \n",
    "        # Isolate the variables stored behind the scenes by the metric operation\n",
    "        running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"accuracy\")\n",
    "        # Define initializer to initialize/reset running variables\n",
    "        self.running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "        \n",
    "        #print(\"first layer: input {}, output {}\".format(self.input_dim, FLAGS.hidden1))\n",
    "        \n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=FLAGS.hidden2,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "    \n",
    "        #print(\"second layer: input {}, output {}\".format(FLAGS.hidden1, FLAGS.hidden2))\n",
    "        \n",
    "        ####### flatten the output so that it is now one continuous layer\n",
    "        #####self.layers.append(Flatten(logging=self.logging))\n",
    "        \n",
    "        # perform self attention, hidden must be number of features, or hidden2. Other two are more arbitrary\n",
    "        self.layers.append(SelfAttention(attention_dim = FLAGS.attention_dim,\n",
    "                                         bias_dim = FLAGS.attention_bias, \n",
    "                                         hidden_units = FLAGS.hidden2,\n",
    "                                         placeholders=self.placeholders,\n",
    "                                         dropout=True,\n",
    "                                         logging = self.logging))\n",
    "        \n",
    "        #print(\"self attention with attention_dim {}, bias_dim {}:\\n input {}, output(bias*input) {}\".format(FLAGS.attention_dim, FLAGS.attention_bias, [self.number_nodes, FLAGS.hidden2] ,FLAGS.hidden2 * FLAGS.attention_bias))\n",
    "        \n",
    "        # dense FC layer to get out an output prediction\n",
    "        self.layers.append(Dense(input_dim=FLAGS.hidden2 * FLAGS.attention_bias,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 act=lambda x: x,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "        \n",
    "        #print(\"final layer: input {}, output {}\".format(FLAGS.hidden2 * FLAGS.attention_bias, self.output_dim))\n",
    "        \n",
    "    def predict(self):\n",
    "        logits = self.outputs\n",
    "        dim = logits.get_shape().as_list()[2]\n",
    "        logits = tf.reshape(logits, [-1, dim])\n",
    "        return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%file utils.py\n",
    "# %load utils.py\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => arr of the feature vectors of the training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.y => arr of the one-hot labels of the labeled training instances as numpy.ndarray object (|label| = number of classes); \n",
    "    ind.dataset_str.graph => arr of adjacency matrices as numpy objects\n",
    "    ind.dataset_str.test.index => index file for test values. To ensure we properly do ONE split for all possible hyperparameters\n",
    "    it simply is in Data/ind.all.test.index. This is NOT regenerated\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(\"..\")\n",
    "    names = ['x', 'y', 'graph', 'sequences', 'labelorder']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"Data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x_arr, y_arr, graph_arr, sequences, labelorder = tuple(objects)\n",
    "    \n",
    "    # this is the tensor of datapoints converted to a list of sparse matrices (BATCHxNxF)\n",
    "    features = x_arr\n",
    "    \n",
    "    # make all the adjacency lists into nx graph objects (BATCHxGRAPHS)\n",
    "    adj_ls = graph_arr\n",
    "    \n",
    "    # read in the test indices from the index file\n",
    "    test_idx_reorder = parse_index_file(\"Data/ind.all.test.index\")\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    \n",
    "    # get training indexes and then split this group up into testing and validation\n",
    "    idx_train = [y_ind for y_ind in range(y_arr.shape[0]) if y_ind not in idx_test]\n",
    "    np.random.shuffle(idx_train)\n",
    "    cutoff = int(6*len(idx_train)/7)\n",
    "    idx_val = idx_train[cutoff:]\n",
    "    idx_train = idx_train[:cutoff]\n",
    "    idx_train, idx_val = np.sort(idx_train), np.sort(idx_val)\n",
    "    idx_test = np.array(idx_test)\n",
    "    \n",
    "    # make logical indices (they are the size BATCH)\n",
    "    train_mask = sample_mask(idx_train, y_arr.shape[0])\n",
    "    val_mask = sample_mask(idx_val, y_arr.shape[0])\n",
    "    test_mask = sample_mask(idx_test, y_arr.shape[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"|Training| {}, |Validation| {}, |Testing| {}\".format(len(idx_train), len(idx_val), len(idx_test)))\n",
    "    \n",
    "    return adj_ls, features, y_arr, sequences, labelorder, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    for i in range(features.shape[0]):\n",
    "        feature_arr = features[i,:,:]\n",
    "        rowsum = np.array(feature_arr.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = np.diag(r_inv)\n",
    "        features[i,:,:] = r_mat_inv.dot(feature_arr)\n",
    "    return features\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    # added a shift to be non negative\n",
    "    adj += np.amax(adj)\n",
    "    # normalize\n",
    "    rowsum = adj.sum(1)\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + np.identity(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support']: support})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = np.identity(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM') # should still work\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - np.identity(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(np.identity(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%file train.py\n",
    "# %load train.py\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from models import GCN\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'testset', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1 (graph conv layer).')\n",
    "flags.DEFINE_integer('hidden2', 7, 'Number of units in hidden layer 2 (graph conv layer).')\n",
    "flags.DEFINE_integer('attention_bias', 2, 'Attention Bias.')\n",
    "flags.DEFINE_integer('attention_dim', 5, 'Attention Dimension.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "flags.DEFINE_string('save_validation', \"False\", \"If you should save validation accuracy\")\n",
    "flags.DEFINE_string('save_test', \"False\", \"If this is optimized run! Use all data and save outputs\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "adj_ls, features, y_arr, sequences, labelorder, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\n",
    "#print(\"Train {}\".format(train_mask))\n",
    "#print(\"Val {}\".format(val_mask))\n",
    "#print(\"Test {}\\n\".format(test_mask))\n",
    "\n",
    "# Save with a name defined by model params\n",
    "model_desc = \"hidden1_{0}_hidden2_{1}_dropout_{2}_attdim_{3}_attbias_{4}_model_{5}_maxdeg_{6}\"\n",
    "model_desc = model_desc.format(FLAGS.hidden1, FLAGS.hidden2, FLAGS.dropout, FLAGS.attention_dim,\n",
    "                              FLAGS.attention_bias, FLAGS.model, FLAGS.max_degree)\n",
    "\n",
    "# determine num_supports and make model function\n",
    "if FLAGS.model == 'gcn':\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# save validation\n",
    "save_validation = FLAGS.save_validation\n",
    "if save_validation == \"True\":\n",
    "    save_validation = True\n",
    "else:\n",
    "    save_validation = False\n",
    "\n",
    "# see if we are saving the output by considering testing set\n",
    "save_test = FLAGS.save_test\n",
    "if save_test == \"True\":\n",
    "    save_test = True\n",
    "else:\n",
    "    save_test = False\n",
    "\n",
    "if save_test:\n",
    "    epoch_df = pd.DataFrame(np.zeros(shape = (FLAGS.epochs, 5)))\n",
    "    epoch_df.columns = [\"train_loss\", \"train_acc\", \"val_acc\", \"val_loss\", \"time\"]\n",
    "    labels_df = pd.DataFrame(np.zeros(shape = (sum(test_mask), 5)))\n",
    "    labels_df.columns = [\"Sequence\", \"Label\", \"Prediction\", \"Negative Class Logit\", \"Positive Class Logit\"]\n",
    "    \n",
    "\n",
    "# preload support tensor so that it isn't needlessly calculated many times\n",
    "batch,_,N,M = adj_ls.shape\n",
    "support_tensor = np.zeros(shape=(batch,num_supports,N,N,M)) # of shape (Batch,Num_Supports,Num_Nodes,Num_Nodes)\n",
    "print(\"Calculating Chebyshev polynomials up to order {}...\".format(FLAGS.max_degree))\n",
    "for b in range(batch):\n",
    "    adj = adj_ls[b]\n",
    "    for m in range(M):\n",
    "        adj = adj_ls[b][:,:,m] # first adjacency list\n",
    "        if FLAGS.model == 'gcn':\n",
    "            support = [preprocess_adj(adj)]\n",
    "        elif FLAGS.model == 'gcn_cheby':\n",
    "            support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "        # add NxN matrices along the num_supports dimension\n",
    "        sup = np.stack(support, axis=0)\n",
    "        # add num_supportsxNxN to support tensor\n",
    "        support_tensor[b,:,:,:,m] = sup\n",
    "\n",
    "# normalize all features\n",
    "features = preprocess_features(features)\n",
    "\n",
    "# Define placeholders\n",
    "F = features.shape[2]\n",
    "placeholders = {\n",
    "    'support': tf.placeholder(tf.float32, shape=(None,num_supports,N,N,M)), # ?xnum_supportsxNxNxM\n",
    "    'features': tf.placeholder(tf.float32, shape=(None,N,F)), # ?xNxF\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_arr.shape[1])), # ?,|labels|\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features.shape[2], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders, model):\n",
    "    t_test = time.time()\n",
    "    features = features[mask,:,:]\n",
    "    support = support[mask,:,:,:]\n",
    "    labels = labels[mask, :]\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(model.running_vars_initializer)\n",
    "\n",
    "# Train model\n",
    "ttot = time.time()\n",
    "cost_val = []\n",
    "acc_val = []\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t = time.time()\n",
    "\n",
    "    # instantiate all inputs\n",
    "    features_train = features[test_mask,:,:]\n",
    "    support = support_tensor[test_mask,:,:,:]\n",
    "    y_test = y_arr[test_mask, :]\n",
    "\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features_train, support, y_test, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Reset the counters\n",
    "    sess.run(model.running_vars_initializer)\n",
    "    \n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Reset the counters\n",
    "    sess.run(model.running_vars_initializer)\n",
    "    \n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support_tensor, y_arr, val_mask, placeholders, model)\n",
    "\n",
    "    cost_val.append(cost)\n",
    "    \n",
    "    # save training progression\n",
    "    if save_test:\n",
    "        epoch_df.iloc[epoch, :] = [outs[1], outs[2], cost, acc, time.time() - t]\n",
    "    \n",
    "    # Print results\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping + 1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished! Total Time: {} sec\".format(time.time() - ttot))\n",
    "\n",
    "# Reset the counters\n",
    "sess.run(model.running_vars_initializer)\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support_tensor, y_arr, test_mask, placeholders, model)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "# save a text file whose name is the model_desc, graph_desc (from dataset), and has this info within it  \n",
    "if save_validation:\n",
    "    root = os.getcwd()\n",
    "    optimization = os.path.join(root, \"Optimization\")\n",
    "    validation_accuracy = acc\n",
    "    filename = \"{0:.5f}.{1}.{2}.outcome.txt\".format(validation_accuracy, model_desc, FLAGS.dataset)\n",
    "    filename = os.path.join(optimization, filename)\n",
    "    with open(filename, \"w\") as fh:\n",
    "        fh.write(\"{}\\n{}\\n{}\".format(validation_accuracy, model_desc, FLAGS.dataset))\n",
    "    \n",
    "\n",
    "# Saving results to a file\n",
    "if save_test:\n",
    "    # get test values\n",
    "    features_test = features[test_mask,:,:]\n",
    "    support_test = support_tensor[test_mask,:,:,:]\n",
    "    labels_test = y_arr[test_mask, :]\n",
    "    \n",
    "    # add sequences\n",
    "    labels_df.iloc[:, 0] = [sequences[i] for i in range(len(test_mask)) if test_mask[i]]\n",
    "    \n",
    "    # add true labels\n",
    "    labels_df.iloc[:, 1] = [np.where(labels_test[i])[0] for i in range((sum(test_mask)))]\n",
    "    \n",
    "    # get logits in final layer\n",
    "    feed_dict_val = construct_feed_dict(features_test, support_test, labels_test, placeholders)\n",
    "    logits, predictions = sess.run([model.logits, model.predictions], feed_dict=feed_dict_val)\n",
    "    labels_df.iloc[:, 3:5] = logits\n",
    "    \n",
    "    # get predictions\n",
    "    labels_df.iloc[:, 2] = predictions\n",
    "    \n",
    "    # change indices for labels to their names\n",
    "    labels_df.iloc[:,1] = labels_df.iloc[:,1].map(lambda x: labelorder[x])\n",
    "    labels_df.iloc[:,2] = labels_df.iloc[:,2].map(lambda x: labelorder[x])\n",
    "    \n",
    "    # write to file\n",
    "    epoch_df.to_csv(\"../Results/{}.{}.epoch.csv\".format(model_desc, FLAGS.dataset), index = False)\n",
    "    labels_df.to_csv(\"../Results/{}.{}.predictions.csv\".format(model_desc, FLAGS.dataset), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Training| 6, |Validation| 1, |Testing| 2\n",
      "False\n",
      "Calculating Chebyshev polynomials up to order 3...\n",
      "2019-06-30 22:41:32.372531: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Epoch: 0020 train_loss= 0.01844 train_acc= 1.00000 val_loss= 0.01817 val_acc= 1.00000 time= 0.00826\n",
      "Epoch: 0040 train_loss= 0.01453 train_acc= 1.00000 val_loss= 0.01438 val_acc= 1.00000 time= 0.00632\n",
      "Epoch: 0060 train_loss= 0.01181 train_acc= 1.00000 val_loss= 0.01169 val_acc= 1.00000 time= 0.00631\n",
      "Epoch: 0080 train_loss= 0.00976 train_acc= 1.00000 val_loss= 0.00967 val_acc= 1.00000 time= 0.00387\n",
      "Epoch: 0100 train_loss= 0.00817 train_acc= 1.00000 val_loss= 0.00810 val_acc= 1.00000 time= 0.00651\n",
      "/home/samuelstentz/.conda/envs/pyrosetta/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/samuelstentz/.conda/envs/pyrosetta/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch: 0120 train_loss= 0.00689 train_acc= 1.00000 val_loss= 0.00683 val_acc= 1.00000 time= 0.00777\n",
      "Epoch: 0140 train_loss= 0.00585 train_acc= 1.00000 val_loss= 0.00581 val_acc= 1.00000 time= 0.00818\n",
      "Epoch: 0160 train_loss= 0.00500 train_acc= 1.00000 val_loss= 0.00496 val_acc= 1.00000 time= 0.00704\n",
      "Epoch: 0180 train_loss= 0.00428 train_acc= 1.00000 val_loss= 0.00425 val_acc= 1.00000 time= 0.00501\n",
      "Epoch: 0200 train_loss= 0.00369 train_acc= 1.00000 val_loss= 0.00366 val_acc= 1.00000 time= 0.00618\n",
      "Epoch: 0220 train_loss= 0.00318 train_acc= 1.00000 val_loss= 0.00316 val_acc= 1.00000 time= 0.00652\n",
      "Epoch: 0240 train_loss= 0.00276 train_acc= 1.00000 val_loss= 0.00274 val_acc= 1.00000 time= 0.00576\n",
      "Epoch: 0260 train_loss= 0.00239 train_acc= 1.00000 val_loss= 0.00238 val_acc= 1.00000 time= 0.00825\n",
      "Epoch: 0280 train_loss= 0.00208 train_acc= 1.00000 val_loss= 0.00207 val_acc= 1.00000 time= 0.00732\n",
      "Epoch: 0300 train_loss= 0.00182 train_acc= 1.00000 val_loss= 0.00180 val_acc= 1.00000 time= 0.00531\n",
      "Epoch: 0320 train_loss= 0.00159 train_acc= 1.00000 val_loss= 0.00158 val_acc= 1.00000 time= 0.00879\n",
      "Epoch: 0340 train_loss= 0.00139 train_acc= 1.00000 val_loss= 0.00138 val_acc= 1.00000 time= 0.00885\n",
      "Epoch: 0360 train_loss= 0.00122 train_acc= 1.00000 val_loss= 0.00121 val_acc= 1.00000 time= 0.00599\n",
      "Epoch: 0380 train_loss= 0.00107 train_acc= 1.00000 val_loss= 0.00106 val_acc= 1.00000 time= 0.00490\n",
      "Epoch: 0400 train_loss= 0.00094 train_acc= 1.00000 val_loss= 0.00094 val_acc= 1.00000 time= 0.00597\n",
      "Epoch: 0420 train_loss= 0.00083 train_acc= 1.00000 val_loss= 0.00083 val_acc= 1.00000 time= 0.00518\n",
      "Epoch: 0440 train_loss= 0.00074 train_acc= 1.00000 val_loss= 0.00073 val_acc= 1.00000 time= 0.00604\n",
      "Epoch: 0460 train_loss= 0.00065 train_acc= 1.00000 val_loss= 0.00065 val_acc= 1.00000 time= 0.00679\n",
      "Epoch: 0480 train_loss= 0.00058 train_acc= 1.00000 val_loss= 0.00058 val_acc= 1.00000 time= 0.00601\n",
      "Epoch: 0500 train_loss= 0.00052 train_acc= 1.00000 val_loss= 0.00052 val_acc= 1.00000 time= 0.00593\n",
      "Epoch: 0520 train_loss= 0.00046 train_acc= 1.00000 val_loss= 0.00046 val_acc= 1.00000 time= 0.00554\n",
      "Epoch: 0540 train_loss= 0.00041 train_acc= 1.00000 val_loss= 0.00041 val_acc= 1.00000 time= 0.00602\n",
      "Epoch: 0560 train_loss= 0.00037 train_acc= 1.00000 val_loss= 0.00037 val_acc= 1.00000 time= 0.00621\n",
      "Epoch: 0580 train_loss= 0.00033 train_acc= 1.00000 val_loss= 0.00033 val_acc= 1.00000 time= 0.00657\n",
      "Epoch: 0600 train_loss= 0.00030 train_acc= 1.00000 val_loss= 0.00030 val_acc= 1.00000 time= 0.00587\n",
      "Epoch: 0620 train_loss= 0.00027 train_acc= 1.00000 val_loss= 0.00027 val_acc= 1.00000 time= 0.00858\n",
      "Epoch: 0640 train_loss= 0.00024 train_acc= 1.00000 val_loss= 0.00024 val_acc= 1.00000 time= 0.00916\n",
      "Epoch: 0660 train_loss= 0.00022 train_acc= 1.00000 val_loss= 0.00022 val_acc= 1.00000 time= 0.00702\n",
      "Epoch: 0680 train_loss= 0.00020 train_acc= 1.00000 val_loss= 0.00020 val_acc= 1.00000 time= 0.00842\n",
      "Epoch: 0700 train_loss= 0.00018 train_acc= 1.00000 val_loss= 0.00018 val_acc= 1.00000 time= 0.00648\n",
      "Epoch: 0720 train_loss= 0.00017 train_acc= 1.00000 val_loss= 0.00016 val_acc= 1.00000 time= 0.00402\n",
      "Epoch: 0740 train_loss= 0.00015 train_acc= 1.00000 val_loss= 0.00015 val_acc= 1.00000 time= 0.00373\n",
      "Epoch: 0760 train_loss= 0.00014 train_acc= 1.00000 val_loss= 0.00014 val_acc= 1.00000 time= 0.00463\n",
      "Epoch: 0780 train_loss= 0.00013 train_acc= 1.00000 val_loss= 0.00013 val_acc= 1.00000 time= 0.00551\n",
      "Epoch: 0800 train_loss= 0.00012 train_acc= 1.00000 val_loss= 0.00012 val_acc= 1.00000 time= 0.00619\n",
      "Epoch: 0820 train_loss= 0.00011 train_acc= 1.00000 val_loss= 0.00011 val_acc= 1.00000 time= 0.00899\n",
      "Epoch: 0840 train_loss= 0.00010 train_acc= 1.00000 val_loss= 0.00010 val_acc= 1.00000 time= 0.00960\n",
      "Epoch: 0860 train_loss= 0.00010 train_acc= 1.00000 val_loss= 0.00009 val_acc= 1.00000 time= 0.00523\n",
      "Epoch: 0880 train_loss= 0.00009 train_acc= 1.00000 val_loss= 0.00009 val_acc= 1.00000 time= 0.00696\n",
      "Epoch: 0900 train_loss= 0.00008 train_acc= 1.00000 val_loss= 0.00008 val_acc= 1.00000 time= 0.00520\n",
      "Epoch: 0920 train_loss= 0.00008 train_acc= 1.00000 val_loss= 0.00008 val_acc= 1.00000 time= 0.00560\n",
      "Epoch: 0940 train_loss= 0.00007 train_acc= 1.00000 val_loss= 0.00007 val_acc= 1.00000 time= 0.00530\n",
      "Epoch: 0960 train_loss= 0.00007 train_acc= 1.00000 val_loss= 0.00007 val_acc= 1.00000 time= 0.00632\n",
      "Epoch: 0980 train_loss= 0.00006 train_acc= 1.00000 val_loss= 0.00006 val_acc= 1.00000 time= 0.00603\n",
      "Epoch: 1000 train_loss= 0.00006 train_acc= 1.00000 val_loss= 0.00006 val_acc= 1.00000 time= 0.00449\n",
      "Epoch: 1020 train_loss= 0.00006 train_acc= 1.00000 val_loss= 0.00006 val_acc= 1.00000 time= 0.00522\n",
      "Epoch: 1040 train_loss= 0.00005 train_acc= 1.00000 val_loss= 0.00005 val_acc= 1.00000 time= 0.00543\n",
      "Epoch: 1060 train_loss= 0.00005 train_acc= 1.00000 val_loss= 0.00005 val_acc= 1.00000 time= 0.00529\n",
      "Epoch: 1080 train_loss= 0.00005 train_acc= 1.00000 val_loss= 0.00005 val_acc= 1.00000 time= 0.00491\n",
      "Epoch: 1100 train_loss= 0.00005 train_acc= 1.00000 val_loss= 0.00005 val_acc= 1.00000 time= 0.00587\n",
      "Epoch: 1120 train_loss= 0.00004 train_acc= 1.00000 val_loss= 0.00004 val_acc= 1.00000 time= 0.00478\n",
      "Epoch: 1140 train_loss= 0.00004 train_acc= 1.00000 val_loss= 0.00004 val_acc= 1.00000 time= 0.00591\n",
      "Epoch: 1160 train_loss= 0.00004 train_acc= 1.00000 val_loss= 0.00004 val_acc= 1.00000 time= 0.00667\n",
      "Epoch: 1180 train_loss= 0.00004 train_acc= 1.00000 val_loss= 0.00004 val_acc= 1.00000 time= 0.00792\n",
      "Epoch: 1200 train_loss= 0.00004 train_acc= 1.00000 val_loss= 0.00004 val_acc= 1.00000 time= 0.00535\n",
      "Epoch: 1220 train_loss= 0.00004 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00532\n",
      "Epoch: 1240 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00522\n",
      "Epoch: 1260 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00502\n",
      "Epoch: 1280 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00498\n",
      "Epoch: 1300 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00505\n",
      "Epoch: 1320 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00572\n",
      "Epoch: 1340 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00641\n",
      "Epoch: 1360 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00661\n",
      "Epoch: 1380 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00464\n",
      "Epoch: 1400 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00003 val_acc= 1.00000 time= 0.00665\n",
      "Epoch: 1420 train_loss= 0.00003 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00520\n",
      "Epoch: 1440 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.01035\n",
      "Epoch: 1460 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00833\n",
      "Epoch: 1480 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00927\n",
      "Epoch: 1500 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.01173\n",
      "Epoch: 1520 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1540 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00945\n",
      "Epoch: 1560 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00522\n",
      "Epoch: 1580 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00574\n",
      "Epoch: 1600 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00643\n",
      "Epoch: 1620 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00437\n",
      "Epoch: 1640 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00528\n",
      "Epoch: 1660 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00501\n",
      "Epoch: 1680 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00305\n",
      "Epoch: 1700 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00599\n",
      "Epoch: 1720 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00580\n",
      "Epoch: 1740 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00534\n",
      "Epoch: 1760 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00562\n",
      "Epoch: 1780 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00002 val_acc= 1.00000 time= 0.00578\n",
      "Epoch: 1800 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00564\n",
      "Epoch: 1820 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00512\n",
      "Epoch: 1840 train_loss= 0.00002 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00527\n",
      "Epoch: 1860 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00532\n",
      "Epoch: 1880 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00471\n",
      "Epoch: 1900 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00649\n",
      "Epoch: 1920 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00659\n",
      "Epoch: 1940 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00600\n",
      "Epoch: 1960 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00840\n",
      "Epoch: 1980 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00709\n",
      "Epoch: 2000 train_loss= 0.00001 train_acc= 1.00000 val_loss= 0.00001 val_acc= 1.00000 time= 0.00624\n",
      "Optimization Finished! Total Time: 13.386757135391235 sec\n",
      "Test set results: cost= 0.00001 accuracy= 1.00000 time= 0.00251\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py -epochs 2000 -early_stopping 100 -hidden1 20 -hidden2 10 -dropout 0 -attention_dim 10 -attention_bias 5 -model gcn_cheby -max_degree 3 -dataset test -save_test False -save_validation True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Training| 6, |Validation| 1, |Testing| 2\n",
      "Calculating Chebyshev polynomials up to order 3...\n",
      "2019-06-28 14:10:27.554643: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Epoch: 0020 train_loss= 0.02935 train_acc= 1.00000 val_loss= 3.39002 val_acc= 0.00000 time= 0.00168\n",
      "Epoch: 0040 train_loss= 0.00593 train_acc= 1.00000 val_loss= 5.85809 val_acc= 0.00000 time= 0.00160\n",
      "Epoch: 0060 train_loss= 0.01794 train_acc= 1.00000 val_loss= 6.65260 val_acc= 0.00000 time= 0.00346\n",
      "Epoch: 0080 train_loss= 0.00538 train_acc= 1.00000 val_loss= 7.18337 val_acc= 0.00000 time= 0.00269\n",
      "Epoch: 0100 train_loss= 0.00519 train_acc= 1.00000 val_loss= 7.44468 val_acc= 0.00000 time= 0.00282\n",
      "/home/samuelstentz/.conda/envs/pyrosetta/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/samuelstentz/.conda/envs/pyrosetta/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch: 0120 train_loss= 0.00610 train_acc= 1.00000 val_loss= 7.61466 val_acc= 0.00000 time= 0.00308\n",
      "Epoch: 0140 train_loss= 0.00511 train_acc= 1.00000 val_loss= 7.94778 val_acc= 0.00000 time= 0.00229\n",
      "Epoch: 0160 train_loss= 0.00468 train_acc= 1.00000 val_loss= 8.36228 val_acc= 0.00000 time= 0.00282\n",
      "Epoch: 0180 train_loss= 0.00448 train_acc= 1.00000 val_loss= 8.74852 val_acc= 0.00000 time= 0.00305\n",
      "Epoch: 0200 train_loss= 0.00430 train_acc= 1.00000 val_loss= 8.85871 val_acc= 0.00000 time= 0.00274\n",
      "Epoch: 0220 train_loss= 0.00412 train_acc= 1.00000 val_loss= 8.88314 val_acc= 0.00000 time= 0.00209\n",
      "Epoch: 0240 train_loss= 0.00399 train_acc= 1.00000 val_loss= 8.97479 val_acc= 0.00000 time= 0.00293\n",
      "Epoch: 0260 train_loss= 0.00400 train_acc= 1.00000 val_loss= 8.98878 val_acc= 0.00000 time= 0.00180\n",
      "Epoch: 0280 train_loss= 0.00367 train_acc= 1.00000 val_loss= 8.97043 val_acc= 0.00000 time= 0.00338\n",
      "Epoch: 0300 train_loss= 0.00352 train_acc= 1.00000 val_loss= 8.96916 val_acc= 0.00000 time= 0.00297\n",
      "Epoch: 0320 train_loss= 0.00338 train_acc= 1.00000 val_loss= 8.96510 val_acc= 0.00000 time= 0.00166\n",
      "Epoch: 0340 train_loss= 0.00324 train_acc= 1.00000 val_loss= 8.92911 val_acc= 0.00000 time= 0.00390\n",
      "Epoch: 0360 train_loss= 0.00334 train_acc= 1.00000 val_loss= 8.92817 val_acc= 0.00000 time= 0.00275\n",
      "Epoch: 0380 train_loss= 0.00542 train_acc= 1.00000 val_loss= 8.88259 val_acc= 0.00000 time= 0.00166\n",
      "Epoch: 0400 train_loss= 0.00288 train_acc= 1.00000 val_loss= 9.00860 val_acc= 0.00000 time= 0.00261\n",
      "Epoch: 0420 train_loss= 0.00275 train_acc= 1.00000 val_loss= 8.95808 val_acc= 0.00000 time= 0.00263\n",
      "Epoch: 0440 train_loss= 0.00269 train_acc= 1.00000 val_loss= 9.03941 val_acc= 0.00000 time= 0.00169\n",
      "Epoch: 0460 train_loss= 0.00255 train_acc= 1.00000 val_loss= 9.04533 val_acc= 0.00000 time= 0.00202\n",
      "Epoch: 0480 train_loss= 0.00245 train_acc= 1.00000 val_loss= 9.02282 val_acc= 0.00000 time= 0.00368\n",
      "Epoch: 0500 train_loss= 0.00239 train_acc= 1.00000 val_loss= 9.04375 val_acc= 0.00000 time= 0.00202\n",
      "Epoch: 0520 train_loss= 0.00228 train_acc= 1.00000 val_loss= 9.06516 val_acc= 0.00000 time= 0.00257\n",
      "Epoch: 0540 train_loss= 0.00219 train_acc= 1.00000 val_loss= 9.31520 val_acc= 0.00000 time= 0.00288\n",
      "Epoch: 0560 train_loss= 0.00217 train_acc= 1.00000 val_loss= 9.39871 val_acc= 0.00000 time= 0.00295\n",
      "Epoch: 0580 train_loss= 0.00206 train_acc= 1.00000 val_loss= 9.47316 val_acc= 0.00000 time= 0.00325\n",
      "Epoch: 0600 train_loss= 0.00195 train_acc= 1.00000 val_loss= 9.43549 val_acc= 0.00000 time= 0.00287\n",
      "Epoch: 0620 train_loss= 0.00186 train_acc= 1.00000 val_loss= 9.31238 val_acc= 0.00000 time= 0.00256\n",
      "Epoch: 0640 train_loss= 0.00185 train_acc= 1.00000 val_loss= 10.07980 val_acc= 0.00000 time= 0.00163\n",
      "Epoch: 0660 train_loss= 0.00182 train_acc= 1.00000 val_loss= 10.35814 val_acc= 0.00000 time= 0.00323\n",
      "Epoch: 0680 train_loss= 0.00195 train_acc= 1.00000 val_loss= 10.26201 val_acc= 0.00000 time= 0.00210\n",
      "Epoch: 0700 train_loss= 0.00165 train_acc= 1.00000 val_loss= 10.15120 val_acc= 0.00000 time= 0.00344\n",
      "Epoch: 0720 train_loss= 0.00157 train_acc= 1.00000 val_loss= 10.04874 val_acc= 0.00000 time= 0.00332\n",
      "Epoch: 0740 train_loss= 0.00155 train_acc= 1.00000 val_loss= 10.08356 val_acc= 0.00000 time= 0.00288\n",
      "Epoch: 0760 train_loss= 0.00146 train_acc= 1.00000 val_loss= 9.95412 val_acc= 0.00000 time= 0.00281\n",
      "Epoch: 0780 train_loss= 0.00141 train_acc= 1.00000 val_loss= 10.29128 val_acc= 0.00000 time= 0.00325\n",
      "Epoch: 0800 train_loss= 0.00135 train_acc= 1.00000 val_loss= 10.43362 val_acc= 0.00000 time= 0.00259\n",
      "Epoch: 0820 train_loss= 0.00129 train_acc= 1.00000 val_loss= 10.31563 val_acc= 0.00000 time= 0.00179\n",
      "Epoch: 0840 train_loss= 0.00124 train_acc= 1.00000 val_loss= 10.38367 val_acc= 0.00000 time= 0.00252\n",
      "Epoch: 0860 train_loss= 0.00130 train_acc= 1.00000 val_loss= 10.36517 val_acc= 0.00000 time= 0.00267\n",
      "Epoch: 0880 train_loss= 0.00115 train_acc= 1.00000 val_loss= 10.50019 val_acc= 0.00000 time= 0.00264\n",
      "Epoch: 0900 train_loss= 0.00113 train_acc= 1.00000 val_loss= 10.78058 val_acc= 0.00000 time= 0.00299\n",
      "Epoch: 0920 train_loss= 0.00128 train_acc= 1.00000 val_loss= 10.74765 val_acc= 0.00000 time= 0.00277\n",
      "Epoch: 0940 train_loss= 0.00103 train_acc= 1.00000 val_loss= 10.56454 val_acc= 0.00000 time= 0.00251\n",
      "Epoch: 0960 train_loss= 0.00098 train_acc= 1.00000 val_loss= 10.40996 val_acc= 0.00000 time= 0.00260\n",
      "Epoch: 0980 train_loss= 0.00096 train_acc= 1.00000 val_loss= 10.56462 val_acc= 0.00000 time= 0.00269\n",
      "Epoch: 1000 train_loss= 0.00092 train_acc= 1.00000 val_loss= 10.60288 val_acc= 0.00000 time= 0.00293\n",
      "Epoch: 1020 train_loss= 0.00088 train_acc= 1.00000 val_loss= 10.47847 val_acc= 0.00000 time= 0.00284\n",
      "Epoch: 1040 train_loss= 0.00089 train_acc= 1.00000 val_loss= 10.27572 val_acc= 0.00000 time= 0.00250\n",
      "Epoch: 1060 train_loss= 0.00079 train_acc= 1.00000 val_loss= 10.15685 val_acc= 0.00000 time= 0.00233\n",
      "Epoch: 1080 train_loss= 0.00076 train_acc= 1.00000 val_loss= 10.12976 val_acc= 0.00000 time= 0.00340\n",
      "Epoch: 1100 train_loss= 0.00074 train_acc= 1.00000 val_loss= 10.09192 val_acc= 0.00000 time= 0.00284\n",
      "Epoch: 1120 train_loss= 0.00071 train_acc= 1.00000 val_loss= 10.16598 val_acc= 0.00000 time= 0.00259\n",
      "Epoch: 1140 train_loss= 0.00072 train_acc= 1.00000 val_loss= 10.25811 val_acc= 0.00000 time= 0.00299\n",
      "Epoch: 1160 train_loss= 0.00067 train_acc= 1.00000 val_loss= 10.47820 val_acc= 0.00000 time= 0.00179\n",
      "Epoch: 1180 train_loss= 0.00080 train_acc= 1.00000 val_loss= 10.39505 val_acc= 0.00000 time= 0.00236\n",
      "Epoch: 1200 train_loss= 0.00061 train_acc= 1.00000 val_loss= 10.17984 val_acc= 0.00000 time= 0.00451\n",
      "Epoch: 1220 train_loss= 0.00058 train_acc= 1.00000 val_loss= 9.95481 val_acc= 0.00000 time= 0.00291\n",
      "Epoch: 1240 train_loss= 0.00056 train_acc= 1.00000 val_loss= 9.71693 val_acc= 0.00000 time= 0.00206\n",
      "Epoch: 1260 train_loss= 0.00053 train_acc= 1.00000 val_loss= 9.73476 val_acc= 0.00000 time= 0.00265\n",
      "Epoch: 1280 train_loss= 0.00068 train_acc= 1.00000 val_loss= 10.76135 val_acc= 0.00000 time= 0.00346\n",
      "Epoch: 1300 train_loss= 0.00053 train_acc= 1.00000 val_loss= 11.07629 val_acc= 0.00000 time= 0.00276\n",
      "Epoch: 1320 train_loss= 0.00051 train_acc= 1.00000 val_loss= 11.00918 val_acc= 0.00000 time= 0.00262\n",
      "Epoch: 1340 train_loss= 0.00048 train_acc= 1.00000 val_loss= 10.72526 val_acc= 0.00000 time= 0.00245\n",
      "Epoch: 1360 train_loss= 0.00048 train_acc= 1.00000 val_loss= 10.90645 val_acc= 0.00000 time= 0.00263\n",
      "Epoch: 1380 train_loss= 0.00050 train_acc= 1.00000 val_loss= 11.42195 val_acc= 0.00000 time= 0.00272\n",
      "Epoch: 1400 train_loss= 0.00048 train_acc= 1.00000 val_loss= 11.45257 val_acc= 0.00000 time= 0.00338\n",
      "Epoch: 1420 train_loss= 0.00046 train_acc= 1.00000 val_loss= 11.35755 val_acc= 0.00000 time= 0.00299\n",
      "Epoch: 1440 train_loss= 0.00047 train_acc= 1.00000 val_loss= 11.04735 val_acc= 0.00000 time= 0.00372\n",
      "Epoch: 1460 train_loss= 0.00043 train_acc= 1.00000 val_loss= 10.82349 val_acc= 0.00000 time= 0.00310\n",
      "Epoch: 1480 train_loss= 0.00039 train_acc= 1.00000 val_loss= 10.76280 val_acc= 0.00000 time= 0.00347\n",
      "Epoch: 1500 train_loss= 0.00037 train_acc= 1.00000 val_loss= 10.52194 val_acc= 0.00000 time= 0.00374\n",
      "Epoch: 1520 train_loss= 0.00035 train_acc= 1.00000 val_loss= 10.25047 val_acc= 0.00000 time= 0.00381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1540 train_loss= 0.00038 train_acc= 1.00000 val_loss= 10.81698 val_acc= 0.00000 time= 0.00386\n",
      "Epoch: 1560 train_loss= 0.00037 train_acc= 1.00000 val_loss= 11.67504 val_acc= 0.00000 time= 0.00304\n",
      "Epoch: 1580 train_loss= 0.00035 train_acc= 1.00000 val_loss= 11.57590 val_acc= 0.00000 time= 0.00334\n",
      "Epoch: 1600 train_loss= 0.00038 train_acc= 1.00000 val_loss= 12.67013 val_acc= 0.00000 time= 0.00219\n",
      "Epoch: 1620 train_loss= 0.00052 train_acc= 1.00000 val_loss= 12.66233 val_acc= 0.00000 time= 0.00232\n",
      "Epoch: 1640 train_loss= 0.00035 train_acc= 1.00000 val_loss= 12.49781 val_acc= 0.00000 time= 0.00168\n",
      "Epoch: 1660 train_loss= 0.00032 train_acc= 1.00000 val_loss= 12.07646 val_acc= 0.00000 time= 0.00233\n",
      "Epoch: 1680 train_loss= 0.00030 train_acc= 1.00000 val_loss= 11.60845 val_acc= 0.00000 time= 0.00247\n",
      "Epoch: 1700 train_loss= 0.00028 train_acc= 1.00000 val_loss= 11.23469 val_acc= 0.00000 time= 0.00276\n",
      "Epoch: 1720 train_loss= 0.00026 train_acc= 1.00000 val_loss= 10.79869 val_acc= 0.00000 time= 0.00299\n",
      "Epoch: 1740 train_loss= 0.00031 train_acc= 1.00000 val_loss= 10.49511 val_acc= 0.00000 time= 0.00280\n",
      "Epoch: 1760 train_loss= 0.00024 train_acc= 1.00000 val_loss= 10.36179 val_acc= 0.00000 time= 0.00432\n",
      "Epoch: 1780 train_loss= 0.00024 train_acc= 1.00000 val_loss= 10.69362 val_acc= 0.00000 time= 0.00178\n",
      "Epoch: 1800 train_loss= 0.00032 train_acc= 1.00000 val_loss= 10.34497 val_acc= 0.00000 time= 0.00178\n",
      "Epoch: 1820 train_loss= 0.00021 train_acc= 1.00000 val_loss= 10.04568 val_acc= 0.00000 time= 0.00211\n",
      "Epoch: 1840 train_loss= 0.00024 train_acc= 1.00000 val_loss= 10.42119 val_acc= 0.00000 time= 0.00242\n",
      "Epoch: 1860 train_loss= 0.00021 train_acc= 1.00000 val_loss= 10.68709 val_acc= 0.00000 time= 0.00260\n",
      "Epoch: 1880 train_loss= 0.00020 train_acc= 1.00000 val_loss= 10.78866 val_acc= 0.00000 time= 0.00285\n",
      "Epoch: 1900 train_loss= 0.00028 train_acc= 1.00000 val_loss= 12.69268 val_acc= 0.00000 time= 0.00270\n",
      "Epoch: 1920 train_loss= 0.00042 train_acc= 1.00000 val_loss= 15.51611 val_acc= 0.00000 time= 0.00299\n",
      "Epoch: 1940 train_loss= 0.00039 train_acc= 1.00000 val_loss= 15.08185 val_acc= 0.00000 time= 0.00308\n",
      "Epoch: 1960 train_loss= 0.00034 train_acc= 1.00000 val_loss= 14.23555 val_acc= 0.00000 time= 0.00261\n",
      "Epoch: 1980 train_loss= 0.00030 train_acc= 1.00000 val_loss= 13.34065 val_acc= 0.00000 time= 0.00247\n",
      "Epoch: 2000 train_loss= 0.00027 train_acc= 1.00000 val_loss= 12.66079 val_acc= 0.00000 time= 0.00482\n",
      "Optimization Finished! Total Time: 5.753514528274536 sec\n",
      "Test set results: cost= 0.00027 accuracy= 1.00000 time= 0.00073\n",
      "CPU times: user 93.8 ms, sys: 62.5 ms, total: 156 ms\n",
      "Wall time: 8.21 s\n"
     ]
    }
   ],
   "source": [
    "%time !python3 train.py -epochs 2000 -early_stopping 100 -hidden1 20 -hidden2 10 -dropout 0.5 -attention_dim 10 -attention_bias 5 -model gcn -max_degree 3 -dataset selector_k_nearest_ratio_1_params_onehot_distance -save_test False -save_validation True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 15.6 ms, total: 31.2 ms\n",
      "Wall time: 5.58 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%time !python3 consolidate_results.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "original placeholders\n",
    "\n",
    "placeholders and descriptions:\n",
    "support: [<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f0a93ef65f8>]\n",
    "features: SparseTensor(indices=Tensor(\"Placeholder_4:0\", shape=(?, 2), dtype=int64), values=Tensor(\"Placeholder_3:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"Const:0\", shape=(2,), dtype=int64))\n",
    "labels: Tensor(\"Placeholder_5:0\", shape=(?, 7), dtype=float32)\n",
    "labels_mask: Tensor(\"Placeholder_6:0\", dtype=int32)\n",
    "dropout: Tensor(\"PlaceholderWithDefault:0\", shape=(), dtype=float32)\n",
    "num_features_nonzero: Tensor(\"Placeholder_7:0\", dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
