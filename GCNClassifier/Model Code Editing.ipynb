{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     12,
     21,
     29,
     73,
     86,
     132,
     165,
     208,
     218,
     222
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting layers.py\n"
     ]
    }
   ],
   "source": [
    "%%file layers.py\n",
    "# %load layers.py\n",
    "from inits import *\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res    \n",
    "    \n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens a tensor layer.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Flatten, self).__init__(**kwargs)\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        # flatten the tensor to one layer\n",
    "        shape = x.get_shape().as_list()               # a list: [None,...]\n",
    "        dim = np.prod(shape[1:])                   # dim = prod(...)\n",
    "        x_flattened = tf.reshape(x, [-1, dim])        # -1 means \"all\"\n",
    "        return x_flattened\n",
    "    \n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "        # add dummy dimension\n",
    "        x = tf.expand_dims(x, 1) # Batch1M\n",
    "        \n",
    "        # transform\n",
    "        (batch, n, m) = x.get_shape().as_list()\n",
    "        p = self.vars['weights'].get_shape().as_list()[1]\n",
    "        output = tf.reshape(tf.reshape(x, [-1, m]) @ self.vars['weights'], [-1, n, p]) # BatchNM * MP => Batch1P (N should be 1)\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output) # Batch1P\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False,\n",
    "                **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.bias = bias\n",
    "        self.num_nodes = self.support.get_shape().as_list()[2]\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            # make all weight matrices for supports in convolution\n",
    "            for i in range(self.support.get_shape().as_list()[1]):# support: ?xSupportsxNxNxM\n",
    "                for j in range(self.support.get_shape().as_list()[4]):\n",
    "                    tensor_name = 'weights_support_' + str(i) + '_M_' + str(j)\n",
    "                    self.vars[tensor_name] = glorot([input_dim, output_dim], name=tensor_name)\n",
    "            # make vector to do weighted sum of all convolved features (w in SUM(wi*(NxF')) for w in M)\n",
    "            self.vars[\"Features Combination\"] =tf.Variable(tf.random_uniform([self.support.get_shape().as_list()[4]]))\n",
    "            #uniform(self.support.get_shape().as_list()[4], name=\"Features Combination\")\n",
    "            # make bias matrice\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "            \n",
    "        \n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # dropout\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "        # convolve\n",
    "        convolved_features = []\n",
    "        for j in range(self.support.get_shape().as_list()[4]):\n",
    "            temp = []\n",
    "            for i in range(self.support.get_shape().as_list()[1]):\n",
    "                tensor_name = 'weights_support_' + str(i) + '_M_' + str(j)\n",
    "                # BatchNF * FF' weight tensor (num_nodes, |Features'|)\n",
    "                (N, F) = x.get_shape().as_list()[1:]\n",
    "                embed = tf.reshape(x, [-1, F])\n",
    "                pre_sup =  tf.reshape(tf.reshape(x, [-1, F]) @ self.vars[tensor_name], [-1, N, self.output_dim])\n",
    "                (batch, _, F_new) = pre_sup.get_shape().as_list()\n",
    "\n",
    "                # BatchNN * BatchNF' => BatchNF'\n",
    "                support = tf.slice(self.support, [0,i,0,0,j], [-1,1,-1,-1,1]) # get Batch1NN1\n",
    "                support = tf.reshape(support, [-1,N,N]) # reshape to BatchNN\n",
    "                support = support @ pre_sup # now BatchNF'\n",
    "                temp.append(support)\n",
    "            # adds together list of BatchNF' into one BatchNF' for a single original adjacency matrix\n",
    "            convolved_F = tf.add_n(temp)\n",
    "            convolved_features.append(convolved_F)\n",
    "        # stack list into one tensor of shape BatchNF'M\n",
    "        convolved_features = tf.stack(convolved_features, axis = 3)\n",
    "        # do weighted multiplication\n",
    "        convolved_features = tf.multiply(convolved_features, self.vars[\"Features Combination\"])\n",
    "        # sum together to remove 4th dimension\n",
    "        output = tf.reduce_sum(convolved_features, axis = 3)\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias'] # Broadcasting spreads bias across Batch and Node dimensions\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    \"\"\"Self attention layer, input is in ?xNxhidden, output is in ?x(Bias*Hidden). Hidden should correspond \n",
    "    to the number of features nodes have.\"\"\"\n",
    "    \n",
    "    def __init__(self, attention_dim, bias_dim, hidden_units, placeholders, dropout=0., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.A = None\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['Ws'] = tf.Variable(tf.random_uniform([attention_dim, self.hidden_units])) # AttentionxHidden\n",
    "            self.vars['W2'] = tf.Variable(tf.random_uniform([bias_dim, attention_dim])) # BiasxAttention\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        \n",
    "        # dropout\n",
    "        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "        \n",
    "        # AttentionxHidden * ?xHiddenxN => ?xAttentionxN\n",
    "        inputsT = tf.transpose(inputs, perm = [0, 2, 1]) # transpose the inner matrices which is our intention\n",
    "        \n",
    "        #print(\"inputsT is of shape {}\".format(inputsT.get_shape().as_list()))\n",
    "        #print(\"self.vars[Ws] is of shape {}\".format(self.vars['Ws'].get_shape().as_list()))\n",
    "        \n",
    "        aux = tf.einsum('ah,bhn->ban', self.vars['Ws'], inputsT)\n",
    "        aux = tf.tanh(aux)\n",
    "        \n",
    "        #print(\"aux is of shape {}\".format(aux.get_shape().as_list()))\n",
    "        \n",
    "        \n",
    "        # BiasxAttention * ?xAttentionxN => ?xBiasxN\n",
    "        self.A = tf.einsum('ba,uan->ubn',self.vars['W2'], aux)\n",
    "        self.A = tf.nn.softmax(self.A)\n",
    "        #print(\"A is of shape {}\".format(self.A.get_shape().as_list()))\n",
    "        #tf.summary.histogram('self_attention', self.A) For visualization\n",
    "        \n",
    "        # ?xBiasxN * ?xNxHidden => ?xBiasxHidden\n",
    "        out = self.A @ inputs\n",
    "        #print(\"out is of shape {}\".format(out.get_shape().as_list()))\n",
    "        \n",
    "        # ?xBiasxHidden => ?x(Bias*Hidden)\n",
    "        out = tf.reshape(out, [ -1, out.get_shape().as_list()[1] * out.get_shape().as_list()[2]])\n",
    "        #print(\"out is of shape {}\".format(out.get_shape().as_list()))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     8,
     88,
     89
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models.py\n"
     ]
    }
   ],
   "source": [
    "%%file models.py\n",
    "# %load models.py\n",
    "from layers import *\n",
    "from metrics import *\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "        self.logits = None\n",
    "        self.predictions = None\n",
    "        \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        #print(\"\\n\\n\\nactivations\\n{}\\n\\n\\n\".format(self.activations))\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        \n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "        self.number_nodes = placeholders['features'].get_shape().as_list()[1]\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        \n",
    "        #print(\"placeholders and descriptions:\")\n",
    "        #for key in self.placeholders:\n",
    "        #    print(\"{}: {}\".format(key, self.placeholders[key]))\n",
    "        \n",
    "        #print(\"Model's input {}, output {}\".format(self.input_dim, self.output_dim))\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "        \n",
    "        # Cross entropy error\n",
    "        labels = self.placeholders['labels']\n",
    "        logits = self.outputs\n",
    "        dim = logits.get_shape().as_list()[2]\n",
    "        logits = tf.reshape(logits, [-1, dim])\n",
    "        entropies = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels, dim = -1)\n",
    "        loss = tf.reduce_mean(entropies)\n",
    "        #print(\"Labels {} \\nLogits {}\\nEntropies {}\\nLoss {}\\n\\n\".format(labels, logits, entropies, loss))\n",
    "        self.loss += loss\n",
    "\n",
    "    def _accuracy(self):\n",
    "        labels = self.placeholders['labels']\n",
    "        logits = self.outputs\n",
    "        dim = logits.get_shape().as_list()[2]\n",
    "        logits = tf.reshape(logits, [-1, dim])\n",
    "        self.logits = logits\n",
    "        labels=tf.argmax(labels, 1) # labels\n",
    "        predictions=tf.argmax(logits, 1) # prediction as one hot\n",
    "        self.predictions = predictions\n",
    "        \n",
    "        # Define the metric and update operations\n",
    "        tf_metric, tf_metric_update = tf.metrics.accuracy(predictions = predictions, labels = labels, name = \"accuracy\")\n",
    "        self.accuracy = tf_metric_update\n",
    "        \n",
    "        # Isolate the variables stored behind the scenes by the metric operation\n",
    "        running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"accuracy\")\n",
    "        # Define initializer to initialize/reset running variables\n",
    "        self.running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "        \n",
    "        #print(\"first layer: input {}, output {}\".format(self.input_dim, FLAGS.hidden1))\n",
    "        \n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=FLAGS.hidden2,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "    \n",
    "        #print(\"second layer: input {}, output {}\".format(FLAGS.hidden1, FLAGS.hidden2))\n",
    "        \n",
    "        ####### flatten the output so that it is now one continuous layer\n",
    "        #####self.layers.append(Flatten(logging=self.logging))\n",
    "        \n",
    "        # perform self attention, hidden must be number of features, or hidden2. Other two are more arbitrary\n",
    "        self.layers.append(SelfAttention(attention_dim = FLAGS.attention_dim,\n",
    "                                         bias_dim = FLAGS.attention_bias, \n",
    "                                         hidden_units = FLAGS.hidden2,\n",
    "                                         placeholders=self.placeholders,\n",
    "                                         dropout=True,\n",
    "                                         logging = self.logging))\n",
    "        \n",
    "        #print(\"self attention with attention_dim {}, bias_dim {}:\\n input {}, output(bias*input) {}\".format(FLAGS.attention_dim, FLAGS.attention_bias, [self.number_nodes, FLAGS.hidden2] ,FLAGS.hidden2 * FLAGS.attention_bias))\n",
    "        \n",
    "        # dense FC layer to get out an output prediction\n",
    "        self.layers.append(Dense(input_dim=FLAGS.hidden2 * FLAGS.attention_bias,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 act=lambda x: x,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "        \n",
    "        #print(\"final layer: input {}, output {}\".format(FLAGS.hidden2 * FLAGS.attention_bias, self.output_dim))\n",
    "        \n",
    "    def predict(self):\n",
    "        logits = self.outputs\n",
    "        dim = logits.get_shape().as_list()[2]\n",
    "        logits = tf.reshape(logits, [-1, dim])\n",
    "        return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%file utils.py\n",
    "# %load utils.py\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => arr of the feature vectors of the training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.y => arr of the one-hot labels of the labeled training instances as numpy.ndarray object (|label| = number of classes); \n",
    "    ind.dataset_str.graph => arr of adjacency matrices as numpy objects\n",
    "    ind.dataset_str.test.index => index file for test values. To ensure we properly do ONE split for all possible hyperparameters\n",
    "    it simply is in Data/ind.all.test.index. This is NOT regenerated\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(\"..\")\n",
    "    names = ['x', 'y', 'graph', 'sequences', 'labelorder']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"Data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x_arr, y_arr, graph_arr, sequences, labelorder = tuple(objects)\n",
    "    \n",
    "    # this is the tensor of datapoints converted to a list of sparse matrices (BATCHxNxF)\n",
    "    features = x_arr\n",
    "    \n",
    "    # make all the adjacency lists into nx graph objects (BATCHxGRAPHS)\n",
    "    adj_ls = graph_arr\n",
    "    \n",
    "    # read in the test indices from the index file\n",
    "    test_idx_reorder = parse_index_file(\"Data/ind.all.test.index\")\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    \n",
    "    # get training indexes and then split this group up into testing and validation\n",
    "    idx_train = [y_ind for y_ind in range(y_arr.shape[0]) if y_ind not in idx_test]\n",
    "    np.random.shuffle(idx_train)\n",
    "    cutoff = int(6*len(idx_train)/7)\n",
    "    idx_val = idx_train[cutoff:]\n",
    "    idx_train = idx_train[:cutoff]\n",
    "    idx_train, idx_val = np.sort(idx_train), np.sort(idx_val)\n",
    "    idx_test = np.array(idx_test)\n",
    "    \n",
    "    # make logical indices (they are the size BATCH)\n",
    "    train_mask = sample_mask(idx_train, y_arr.shape[0])\n",
    "    val_mask = sample_mask(idx_val, y_arr.shape[0])\n",
    "    test_mask = sample_mask(idx_test, y_arr.shape[0])\n",
    "\n",
    "    return adj_ls, features, y_arr, sequences, labelorder, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    for i in range(features.shape[0]):\n",
    "        feature_arr = features[i,:,:]\n",
    "        rowsum = np.array(feature_arr.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = np.diag(r_inv)\n",
    "        features[i,:,:] = r_mat_inv.dot(feature_arr)\n",
    "    return features\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    # added a shift to be non negative\n",
    "    adj += np.amax(adj)\n",
    "    # normalize\n",
    "    rowsum = adj.sum(1)\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + np.identity(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support']: support})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = np.identity(adj.shape[0]) - adj_normalized\n",
    "    try:\n",
    "        largest_eigval, _ = eigsh(laplacian, 1, which='LM') # should still work\n",
    "    except:\n",
    "        largest_eigval, _ = eigsh(laplacian, 1, which='LM') # should still work, some wierd bug\n",
    "        \n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - np.identity(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(np.identity(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     109,
     209
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%file train.py\n",
    "# %load train.py\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from models import GCN\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'wholeset', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1 (graph conv layer).')\n",
    "flags.DEFINE_integer('hidden2', 7, 'Number of units in hidden layer 2 (graph conv layer).')\n",
    "flags.DEFINE_integer('attention_bias', 2, 'Attention Bias.')\n",
    "flags.DEFINE_integer('attention_dim', 5, 'Attention Dimension.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "flags.DEFINE_string('save_validation', \"False\", \"If you should save validation accuracy\")\n",
    "flags.DEFINE_string('save_test', \"False\", \"If this is optimized run! Use all data and save outputs\")\n",
    "flags.DEFINE_integer('k-fold', -1, \"If this run is a k-folded run! If given save_val, save_test, etc are all ignored\")\n",
    "flags.DEFINE_string('test_dataset', 'testset', \"If we are testing with a unique test_set\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "adj_ls, features, y_arr, sequences, labelorder, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\n",
    "# Check for independent test_dataset\n",
    "if FLAGS.test_dataset != \"testset\":\n",
    "    adj_ls_test, features_test, y_arr_test, sequences_test, _, _, _, test_mask = load_data(FLAGS.test_dataset)\n",
    "    adj_ls = np.concatenate((adj_ls, adj_ls_test), axis = 0)\n",
    "    features = np.concatenate((features, features_test), axis = 0)\n",
    "    y_arr = np.concatenate((y_arr, y_arr_test), axis = 0)\n",
    "    sequences = sequences + sequences_test\n",
    "    \n",
    "    # make all the indices true and false, then concatenate and invert for test and train\n",
    "    test_mask[0:len(test_mask)] = True\n",
    "    train_mask[0:len(train_mask)] = False\n",
    "\n",
    "    test_mask = np.concatenate((train_mask, test_mask))\n",
    "    train_mask = np.array([not xi for xi in test_mask], dtype = np.bool)\n",
    "    val_mask = np.array([False for xi in test_mask], dtype = np.bool)\n",
    "    \n",
    "\n",
    "#print(\"Train {}\".format(train_mask))\n",
    "#print(\"Val {}\".format(val_mask))\n",
    "#print(\"Test {}\\n\".format(test_mask))\n",
    "\n",
    "print(\"|Training| {}, |Validation| {}, |Testing| {}\".format(np.sum(train_mask), np.sum(val_mask), np.sum(test_mask)))\n",
    "\n",
    "# Save with a name defined by model params\n",
    "model_desc = \"hidden1_{0}_hidden2_{1}_dropout_{2}_attdim_{3}_attbias_{4}_model_{5}_maxdeg_{6}\"\n",
    "model_desc = model_desc.format(FLAGS.hidden1, FLAGS.hidden2, FLAGS.dropout, FLAGS.attention_dim,\n",
    "                              FLAGS.attention_bias, FLAGS.model, FLAGS.max_degree)\n",
    "\n",
    "# determine num_supports and make model function\n",
    "if FLAGS.model == 'gcn':\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# save validation\n",
    "save_validation = FLAGS.save_validation\n",
    "if save_validation == \"True\":\n",
    "    save_validation = True\n",
    "else:\n",
    "    save_validation = False\n",
    "\n",
    "# see if we are saving the output by considering testing set\n",
    "save_test = FLAGS.save_test\n",
    "if save_test == \"True\":\n",
    "    save_test = True\n",
    "else:\n",
    "    save_test = False\n",
    "\n",
    "if save_test:\n",
    "    epoch_df = pd.DataFrame(np.zeros(shape = (FLAGS.epochs, 5)))\n",
    "    epoch_df.columns = [\"train_loss\", \"train_acc\", \"val_acc\", \"val_loss\", \"time\"]\n",
    "    labels_df = pd.DataFrame(np.zeros(shape = (sum(test_mask), 5)))\n",
    "    labels_df.columns = [\"Sequence\", \"Label\", \"Prediction\", \"Negative Class Logit\", \"Positive Class Logit\"]\n",
    "    # add validation to training set for best results\n",
    "    train_mask = np.array([xi or yi for (xi, yi) in zip(train_mask, val_mask)], dtype = np.bool)\n",
    "\n",
    "# initial time\n",
    "ttot = time.time()\n",
    "\n",
    "# preload support tensor so that it isn't needlessly calculated many times\n",
    "batch,_,N,M = adj_ls.shape\n",
    "support_tensor = np.zeros(shape=(batch,num_supports,N,N,M)) # of shape (Batch,Num_Supports,Num_Nodes,Num_Nodes,Num_Edge)\n",
    "print(\"Calculating Chebyshev polynomials up to order {}...\".format(FLAGS.max_degree))\n",
    "for b in range(batch):\n",
    "    adj = adj_ls[b]\n",
    "    for m in range(M):\n",
    "        adj = adj_ls[b][:,:,m] # first adjacency list\n",
    "        if FLAGS.model == 'gcn':\n",
    "            support = [preprocess_adj(adj)]\n",
    "        elif FLAGS.model == 'gcn_cheby':\n",
    "            support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "        # add NxN matrices along the num_supports dimension\n",
    "        sup = np.stack(support, axis=0)\n",
    "        # add num_supportsxNxN to support tensor\n",
    "        support_tensor[b,:,:,:,m] = sup\n",
    "\n",
    "# normalize all features\n",
    "features = preprocess_features(features)\n",
    "\n",
    "# Define placeholders\n",
    "F = features.shape[2]\n",
    "placeholders = {\n",
    "    'support': tf.placeholder(tf.float32, shape=(None,num_supports,N,N,M)), # ?xnum_supportsxNxNxM\n",
    "    'features': tf.placeholder(tf.float32, shape=(None,N,F)), # ?xNxF\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_arr.shape[1])), # ?,|labels|\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features.shape[2], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders, model):\n",
    "    t_test = time.time()\n",
    "    features = features[mask,:,:]\n",
    "    support = support[mask,:,:,:]\n",
    "    labels = labels[mask, :]\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(model.running_vars_initializer)\n",
    "\n",
    "# Train model\n",
    "t = time.time()\n",
    "cost_val = []\n",
    "acc_val = []\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t_epoch = time.time()\n",
    "    \n",
    "    # instantiate all inputs\n",
    "    features_train = features[test_mask,:,:]\n",
    "    support = support_tensor[test_mask,:,:,:]\n",
    "    y_test = y_arr[test_mask, :]\n",
    "\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features_train, support, y_test, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Reset the counters\n",
    "    sess.run(model.running_vars_initializer)\n",
    "    \n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Reset the counters\n",
    "    sess.run(model.running_vars_initializer)\n",
    "    \n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support_tensor, y_arr, val_mask, placeholders, model)\n",
    "    cost_val.append(cost)\n",
    "    \n",
    "    # save training progression\n",
    "    if save_test:\n",
    "        epoch_df.iloc[epoch, :] = [outs[1], outs[2], cost, acc, time.time() - t_epoch]\n",
    "    \n",
    "    # Print results\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        t = time.time()\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping + 1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished! Total Time: {} sec\".format(time.time() - ttot))\n",
    "\n",
    "# Reset the counters\n",
    "sess.run(model.running_vars_initializer)\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support_tensor, y_arr, test_mask, placeholders, model)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "# save a text file whose name is the model_desc, graph_desc (from dataset), and has this info within it  \n",
    "if save_validation:\n",
    "    root = os.getcwd()\n",
    "    optimization = os.path.join(root, \"Optimization\")\n",
    "    validation_accuracy = acc\n",
    "    filename = \"{0:.5f}.{1}.{2}.outcome.txt\".format(validation_accuracy, model_desc, FLAGS.dataset)\n",
    "    filename = os.path.join(optimization, filename)\n",
    "    with open(filename, \"w\") as fh:\n",
    "        fh.write(\"{}\\n{}\\n{}\".format(validation_accuracy, model_desc, FLAGS.dataset))\n",
    "    \n",
    "# Saving results to a file\n",
    "if save_test:\n",
    "    # get test values\n",
    "    features_test = features[test_mask,:,:]\n",
    "    support_test = support_tensor[test_mask,:,:,:]\n",
    "    labels_test = y_arr[test_mask, :]\n",
    "    \n",
    "    # add sequences\n",
    "    labels_df.iloc[:, 0] = [sequences[i] for i in range(len(test_mask)) if test_mask[i]]\n",
    "    \n",
    "    # add true labels\n",
    "    labels_df.iloc[:, 1] = [np.where(labels_test[i])[0] for i in range((sum(test_mask)))]\n",
    "    \n",
    "    # get logits in final layer\n",
    "    feed_dict_val = construct_feed_dict(features_test, support_test, labels_test, placeholders)\n",
    "    logits, predictions = sess.run([model.logits, model.predictions], feed_dict=feed_dict_val)\n",
    "    labels_df.iloc[:, 3:5] = logits\n",
    "    \n",
    "    # get predictions\n",
    "    labels_df.iloc[:, 2] = predictions\n",
    "    \n",
    "    # change indices for labels to their names\n",
    "    labels_df.iloc[:,1] = labels_df.iloc[:,1].map(lambda x: labelorder[x])\n",
    "    labels_df.iloc[:,2] = labels_df.iloc[:,2].map(lambda x: labelorder[x])\n",
    "    \n",
    "    # write to file\n",
    "    epoch_df.to_csv(\"../Results/{}.{}.epoch.csv\".format(model_desc, FLAGS.dataset), index = False)\n",
    "    labels_df.to_csv(\"../Results/{}.{}.predictions.csv\".format(model_desc, FLAGS.dataset), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Training| 996, |Validation| 166, |Testing| 498\n",
      "Calculating Chebyshev polynomials up to order 3...\n",
      "/mnt/c/Users/Owner/Documents/Code/Research_Scripts/PyRosetta/Graph-Convolutional-Neural-Network/GCNClassifier/utils.py:101: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py -epochs 60 -early_stopping 100 -hidden1 20 -hidden2 10 -dropout 0 -attention_dim 10 -attention_bias 5 -model gcn_cheby -max_degree 3 -dataset selector_8_ang_ratio_0_params_all_onehot_distance -save_test True -save_validation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 15.6 ms, total: 15.6 ms\n",
      "Wall time: 767 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%time !python3 consolidate_results.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "original placeholders\n",
    "\n",
    "placeholders and descriptions:\n",
    "support: [<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f0a93ef65f8>]\n",
    "features: SparseTensor(indices=Tensor(\"Placeholder_4:0\", shape=(?, 2), dtype=int64), values=Tensor(\"Placeholder_3:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"Const:0\", shape=(2,), dtype=int64))\n",
    "labels: Tensor(\"Placeholder_5:0\", shape=(?, 7), dtype=float32)\n",
    "labels_mask: Tensor(\"Placeholder_6:0\", dtype=int32)\n",
    "dropout: Tensor(\"PlaceholderWithDefault:0\", shape=(), dtype=float32)\n",
    "num_features_nonzero: Tensor(\"Placeholder_7:0\", dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]]\n",
      "(26, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.]), array([[ 0.06820068],\n",
       "        [ 0.05824877],\n",
       "        [-0.15169597],\n",
       "        [ 0.03231936],\n",
       "        [ 0.0931789 ],\n",
       "        [ 0.00290181],\n",
       "        [ 0.06703742],\n",
       "        [ 0.36320261],\n",
       "        [-0.26347516],\n",
       "        [-0.05083581],\n",
       "        [-0.10620215],\n",
       "        [ 0.25611725],\n",
       "        [-0.12235471],\n",
       "        [ 0.20537812],\n",
       "        [ 0.30986563],\n",
       "        [ 0.24861462],\n",
       "        [-0.13556621],\n",
       "        [ 0.02169414],\n",
       "        [-0.12080429],\n",
       "        [ 0.45722021],\n",
       "        [-0.14662624],\n",
       "        [ 0.0022738 ],\n",
       "        [-0.13244341],\n",
       "        [ 0.02799337],\n",
       "        [ 0.37835751],\n",
       "        [ 0.18293973]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "laplacian = np.identity(26)\n",
    "print(type(laplacian))\n",
    "print(laplacian)\n",
    "print(laplacian.shape)\n",
    "eigsh(laplacian, 1, which='LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
