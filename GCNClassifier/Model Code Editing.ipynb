{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0,
     6,
     11,
     17,
     22
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inits.py\n"
     ]
    }
   ],
   "source": [
    "%%file inits.py\n",
    "# %load inits.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%file utils.py\n",
    "# %load utils.py\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => arr of the feature vectors of the training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.y => arr of the one-hot labels of the labeled training instances as numpy.ndarray object (|label| = number of classes); \n",
    "    ind.dataset_str.graph => arr of adjacency matrices as numpy objects\n",
    "    ind.dataset_str.test.index => index file for test values. To ensure we properly do ONE split for all possible hyperparameters\n",
    "    it simply is in Data/ind.all.test.index. This is NOT regenerated\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(\"..\")\n",
    "    names = ['x', 'y', 'graph', 'sequences', 'labelorder']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"Data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x_arr, y_arr, graph_arr, sequences, labelorder = tuple(objects)\n",
    "    \n",
    "    # this is the tensor of datapoints converted to a list of sparse matrices (BATCHxNxF)\n",
    "    features = x_arr\n",
    "    \n",
    "    # make all the adjacency lists into nx graph objects (BATCHxGRAPHS)\n",
    "    adj_ls = graph_arr\n",
    "    \n",
    "    # read in the test indices from the index file\n",
    "    test_idx_reorder = parse_index_file(\"Data/ind.all.test.index\")\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    \n",
    "    # get training indexes and then split this group up into testing and validation\n",
    "    idx_train = [y_ind for y_ind in range(y_arr.shape[0]) if y_ind not in idx_test]\n",
    "    np.random.shuffle(idx_train)\n",
    "    cutoff = int(6*len(idx_train)/7)\n",
    "    idx_val = idx_train[cutoff:]\n",
    "    idx_train = idx_train[:cutoff]\n",
    "    idx_train, idx_val = np.sort(idx_train), np.sort(idx_val)\n",
    "    idx_test = np.array(idx_test)\n",
    "    \n",
    "    # make logical indices (they are the size BATCH)\n",
    "    train_mask = sample_mask(idx_train, y_arr.shape[0])\n",
    "    val_mask = sample_mask(idx_val, y_arr.shape[0])\n",
    "    test_mask = sample_mask(idx_test, y_arr.shape[0])\n",
    "\n",
    "    return adj_ls, features, y_arr, sequences, labelorder, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    for i in range(features.shape[0]):\n",
    "        feature_arr = features[i,:,:]\n",
    "        rowsum = np.array(feature_arr.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = np.diag(r_inv)\n",
    "        features[i,:,:] = r_mat_inv.dot(feature_arr)\n",
    "    return features\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    # added a shift to be non negative\n",
    "    adj += np.amax(adj)\n",
    "    # normalize\n",
    "    rowsum = adj.sum(1)\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + np.identity(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support']: support})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = np.identity(adj.shape[0]) - adj_normalized\n",
    "    try:\n",
    "        largest_eigval, _ = eigsh(laplacian, 1, which='LM') # should still work\n",
    "    except:\n",
    "        largest_eigval, _ = eigsh(laplacian, 1, which='LM') # should still work, some wierd bug\n",
    "        \n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - np.identity(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(np.identity(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": [
     0,
     12,
     21,
     29,
     73,
     86,
     125,
     127,
     143,
     196,
     200,
     214
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting layers.py\n"
     ]
    }
   ],
   "source": [
    "%%file layers.py\n",
    "# %load layers.py\n",
    "from inits import *\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res    \n",
    "    \n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\"Flattens a tensor layer.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Flatten, self).__init__(**kwargs)\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        # flatten the tensor to one layer\n",
    "        shape = x.get_shape().as_list()               # a list: [None,...]\n",
    "        dim = np.prod(shape[1:])                   # dim = prod(...)\n",
    "        x_flattened = tf.reshape(x, [-1, dim])        # -1 means \"all\"\n",
    "        return x_flattened\n",
    "    \n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "        # transform\n",
    "        output = x @ self.vars['weights']\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "    \n",
    "        return self.act(output) # BatchOutput\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 act=tf.nn.relu, bias=False,\n",
    "                **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.bias = bias\n",
    "        self.num_nodes = self.support.get_shape().as_list()[2]\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            # make all weight matrices for supports in convolution\n",
    "            for i in range(self.support.get_shape().as_list()[1]):# support: ?xSupportsxNxNxM\n",
    "                for j in range(self.support.get_shape().as_list()[4]):\n",
    "                    tensor_name = 'weights_support_' + str(i) + '_M_' + str(j)\n",
    "                    self.vars[tensor_name] = glorot([input_dim, output_dim], name=tensor_name)\n",
    "            # make vector to do weighted sum of all convolved features (w in SUM(wi*(NxF')) for w in M)\n",
    "            self.vars[\"Features Combination\"] = tf.Variable(tf.random_uniform([self.support.get_shape().as_list()[4]]))\n",
    "            # make bias matrice\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "        \n",
    "        # dropout\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "        # convolve\n",
    "        convolved_features = []\n",
    "        for j in range(self.support.get_shape().as_list()[4]):\n",
    "            temp = []\n",
    "            for i in range(self.support.get_shape().as_list()[1]):\n",
    "                tensor_name = 'weights_support_' + str(i) + '_M_' + str(j)\n",
    "                # BatchNF * FF' weight tensor (num_nodes, |Features'|)\n",
    "                (N, F) = x.get_shape().as_list()[1:]\n",
    "                embed = tf.reshape(x, [-1, F])\n",
    "                pre_sup =  tf.reshape(tf.reshape(x, [-1, F]) @ self.vars[tensor_name], [-1, N, self.output_dim])\n",
    "                (batch, _, F_new) = pre_sup.get_shape().as_list()\n",
    "\n",
    "                # BatchNN * BatchNF' => BatchNF'\n",
    "                support = tf.slice(self.support, [0,i,0,0,j], [-1,1,-1,-1,1]) # get Batch1NN1\n",
    "                support = tf.reshape(support, [-1,N,N]) # reshape to BatchNN\n",
    "                support = support @ pre_sup # now BatchNF'\n",
    "                temp.append(support)\n",
    "            # adds together list of BatchNF' into one BatchNF' for a single original adjacency matrix\n",
    "            convolved_F = tf.add_n(temp)\n",
    "            convolved_features.append(convolved_F)\n",
    "        # stack list into one tensor of shape BatchNF'M\n",
    "        convolved_features = tf.stack(convolved_features, axis = 3)\n",
    "        # do weighted multiplication\n",
    "        convolved_features = tf.multiply(convolved_features, self.vars[\"Features Combination\"])\n",
    "        # sum together to remove 4th dimension\n",
    "        output = tf.reduce_sum(convolved_features, axis = 3)\n",
    "        \n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias'] # Broadcasting spreads bias across Batch and Node dimensions\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    \"\"\"Self attention layer, input is in ?xNxhidden, output is in ?x(Bias*Hidden). Hidden should correspond \n",
    "    to the number of features nodes have.\"\"\"\n",
    "    \n",
    "    def __init__(self, attention_dim, bias_dim, hidden_units, placeholders, dropout=0., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.A = None\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['Ws'] = glorot([attention_dim, self.hidden_units])#tf.Variable(tf.random_uniform([attention_dim, self.hidden_units])) # AttentionxHidden\n",
    "            self.vars['W2'] = glorot([bias_dim, attention_dim])#tf.Variable(tf.random_uniform([bias_dim, attention_dim])) # BiasxAttention\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        # dropout\n",
    "        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "        inputsT = tf.transpose(inputs, perm = [0, 2, 1]) # transpose the inner matrices which is our intention\n",
    "        \n",
    "        # AttentionxHidden * ?xHiddenxN => ?xAttentionxN\n",
    "        aux = tf.einsum('ah,bhn->ban', self.vars['Ws'], inputsT)\n",
    "        aux = tf.tanh(aux)\n",
    "        \n",
    "        # BiasxAttention * ?xAttentionxN => ?xBiasxN\n",
    "        self.A = tf.einsum('ba,uan->ubn',self.vars['W2'], aux)\n",
    "        self.A = tf.nn.softmax(self.A)\n",
    "        \n",
    "        # ?xBiasxN * ?xNxHidden => ?xBiasxHidden\n",
    "        out = self.A @ inputs\n",
    "        \n",
    "        # ?xBiasxHidden => ?x(Bias*Hidden)\n",
    "        out = tf.reshape(out, [ -1, out.get_shape().as_list()[1] * out.get_shape().as_list()[2]])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": [
     0,
     100,
     190
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models.py\n"
     ]
    }
   ],
   "source": [
    "%%file models.py\n",
    "# %load models.py\n",
    "from layers import *\n",
    "from metrics import *\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "        self.logits = None\n",
    "        self.predictions = None\n",
    "        self.attentions = None\n",
    "        \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.f1_score = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "        self.number_nodes = placeholders['features'].get_shape().as_list()[1]\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        self.attention_layer = None\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "        \n",
    "        if FLAGS.balanced_training == \"True\":\n",
    "            labels = self.placeholders['labels']\n",
    "            logits = self.outputs\n",
    "            # Get relative frequency of each class\n",
    "            class_counts = tf.reduce_sum(labels, 0)\n",
    "            class_frequencies = class_counts / tf.reduce_sum(class_counts)\n",
    "            # Cross entropy error\n",
    "            entropies = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels, dim = -1)\n",
    "            # Scale by 1/frequency\n",
    "            scalers = labels / class_frequencies\n",
    "            scalers = tf.reduce_sum(scalers, 1)\n",
    "            entropies_scaled = scalers * entropies\n",
    "            self.loss += tf.reduce_mean(entropies_scaled)\n",
    "        else:\n",
    "            # Cross entropy error\n",
    "            labels = self.placeholders['labels']\n",
    "            logits = self.outputs\n",
    "            entropies = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels, dim = -1)\n",
    "            loss = tf.reduce_mean(entropies)\n",
    "            self.loss += loss\n",
    "\n",
    "    def _accuracy(self):\n",
    "        labels = self.placeholders['labels']\n",
    "        logits = self.outputs\n",
    "        self.logits = logits\n",
    "        self.attentions = self.attention_layer.A\n",
    "        labels=tf.argmax(labels, 1) # labels\n",
    "        predictions=tf.argmax(logits, 1) # prediction as one hot\n",
    "        self.predictions = predictions\n",
    "        \n",
    "        # Define the metric and update operations, f1 score is also calculated\n",
    "        tf_metric, tf_metric_update = tf.metrics.accuracy(predictions = predictions, labels = labels, name = \"accuracy\")\n",
    "        self.accuracy = tf_metric_update\n",
    "        \n",
    "        # Isolate the variables stored behind the scenes by the metric operation\n",
    "        running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"accuracy\")\n",
    "        \n",
    "        # Define initializer to initialize/reset running variables\n",
    "        self.running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "\n",
    "    def _build(self):\n",
    "        # Parse through the graph convolutional layer specifications and the hidden layers\n",
    "        def parse_array(string):\n",
    "            sl = list(string)\n",
    "            if sl[0] != \"[\" and sl[-1] == \"]\":\n",
    "                raise ValueError(\"Invalid dimensions input\")\n",
    "            string = string.strip(\"[]\")\n",
    "            string = string.replace(\" \", \"\")\n",
    "            num_ls = string.split(\",\")\n",
    "            return [int(x) for x in num_ls if x != \"\"]\n",
    "        graph_convolution_dimensions = parse_array(FLAGS.graph_conv_dimensions)\n",
    "        fully_connected_dimensions = parse_array(FLAGS.connected_dimensions)\n",
    "        \n",
    "        # Graph Convolutional Layers\n",
    "        prior_dimension = self.input_dim\n",
    "        for gcdim in graph_convolution_dimensions:\n",
    "            self.layers.append(GraphConvolution(input_dim=prior_dimension,\n",
    "                                                output_dim=gcdim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=tf.nn.relu,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "            prior_dimension = gcdim\n",
    "        \n",
    "        # Self Attention\n",
    "        self.layers.append(SelfAttention(attention_dim=FLAGS.attention_dim,\n",
    "                                         bias_dim=FLAGS.attention_bias, \n",
    "                                         hidden_units=prior_dimension,\n",
    "                                         placeholders=self.placeholders,\n",
    "                                         dropout=True,\n",
    "                                         logging=self.logging))\n",
    "        self.attention_layer = self.layers[-1]\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        fully_connected_dimensions.append(self.output_dim)\n",
    "        prior_dimension = prior_dimension * FLAGS.attention_bias\n",
    "        for fcdim in fully_connected_dimensions:\n",
    "            self.layers.append(Dense(input_dim=prior_dimension,\n",
    "                                     output_dim=fcdim,\n",
    "                                     act=tf.nn.relu,\n",
    "                                     placeholders=self.placeholders,\n",
    "                                     dropout=True,\n",
    "                                     logging=self.logging))\n",
    "            prior_dimension = fcdim\n",
    "        \n",
    "    def predict(self):\n",
    "        logits = self.outputs\n",
    "        return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%file train.py\n",
    "# %load train.py\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from models import GCN\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'wholeset', 'Dataset string.')\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_string('graph_conv_dimensions', '[20,20]', 'Number of units in each graph convolution layer.')\n",
    "flags.DEFINE_string('connected_dimensions','[]', 'Number of units in each FC layer.')\n",
    "flags.DEFINE_integer('attention_bias', 2, 'Attention Bias.')\n",
    "flags.DEFINE_integer('attention_dim', 5, 'Attention Dimension.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "flags.DEFINE_string('save_validation', \"False\", \"If you should save validation accuracy\")\n",
    "flags.DEFINE_string('save_test', \"False\", \"If this is a optimized run! Use all data and save outputs\")\n",
    "flags.DEFINE_integer('k-fold', -1, \"If this run is a k-folded run! If given save_val, save_test, etc are all ignored\")\n",
    "flags.DEFINE_string('test_dataset', 'testset', \"If we are testing with a unique test_set\")\n",
    "flags.DEFINE_string('balanced_training', 'False', \"use a weighted classwise loss to prevent favoring larger class\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "adj_ls, features, y_arr, sequences, labelorder, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\n",
    "# Check for independent test_dataset\n",
    "if FLAGS.test_dataset != \"testset\":\n",
    "    adj_ls_test, features_test, y_arr_test, sequences_test, _, _, _, test_mask = load_data(FLAGS.test_dataset)\n",
    "    adj_ls = np.concatenate((adj_ls, adj_ls_test), axis = 0)\n",
    "    features = np.concatenate((features, features_test), axis = 0)\n",
    "    y_arr = np.concatenate((y_arr, y_arr_test), axis = 0)\n",
    "    sequences = sequences + sequences_test\n",
    "    \n",
    "    # make all the indices true and false, then concatenate and invert for test and train\n",
    "    test_mask[0:len(test_mask)] = True\n",
    "    train_mask[0:len(train_mask)] = False\n",
    "\n",
    "    test_mask = np.concatenate((train_mask, test_mask))\n",
    "    train_mask = np.array([not xi for xi in test_mask], dtype = np.bool)\n",
    "    val_mask = np.array([False for xi in test_mask], dtype = np.bool)\n",
    "\n",
    "# Save with a name defined by model params\n",
    "model_desc = \"learningrate_{7}_epochs_{8}__graphconv_{0}_dropout_{1}_attdim_{2}_attbias_{3}_fc_{4}_model_{5}_maxdeg_{6}\"\n",
    "model_desc = model_desc.format(FLAGS.graph_conv_dimensions, FLAGS.dropout, FLAGS.attention_dim,\n",
    "                              FLAGS.attention_bias, FLAGS.connected_dimensions, FLAGS.model, FLAGS.max_degree,\n",
    "                              FLAGS.learning_rate, FLAGS.epochs)\n",
    "\n",
    "# determine num_supports and make model function\n",
    "if FLAGS.model == 'gcn':\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# save validation\n",
    "save_validation = FLAGS.save_validation\n",
    "if save_validation == \"True\": save_validation = True\n",
    "else: save_validation = False\n",
    "\n",
    "# see if we are saving the output by considering testing set\n",
    "save_test = FLAGS.save_test\n",
    "if save_test == \"True\": save_test = True\n",
    "else: save_test = False\n",
    "\n",
    "if save_test:\n",
    "    epoch_df = pd.DataFrame(np.zeros(shape = (FLAGS.epochs, 5)))\n",
    "    labels_df = pd.DataFrame(np.zeros(shape = (sum(test_mask), 5)))\n",
    "    # add validation to training set for best results\n",
    "    train_mask = np.array([xi or yi for (xi, yi) in zip(train_mask, val_mask)], dtype = np.bool)\n",
    "\n",
    "# see size of inputs\n",
    "print(\"|Training| {}, |Validation| {}, |Testing| {}\".format(np.sum(train_mask), np.sum(val_mask), np.sum(test_mask)))\n",
    "\n",
    "# initial time\n",
    "ttot = time.time()\n",
    "\n",
    "# preload support tensor so that it isn't needlessly calculated many times\n",
    "batch,_,N,M = adj_ls.shape\n",
    "support_tensor = np.zeros(shape=(batch,num_supports,N,N,M)) # of shape (Batch,Num_Supports,Num_Nodes,Num_Nodes,Num_Edge)\n",
    "if FLAGS.model == \"gcn_cheby\":\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(FLAGS.max_degree))\n",
    "else:\n",
    "    print(\"Preprocessing adjacency lists\")\n",
    "for b in range(batch):\n",
    "    adj = adj_ls[b]\n",
    "    for m in range(M):\n",
    "        adj = adj_ls[b][:,:,m] # first adjacency list\n",
    "        if FLAGS.model == 'gcn':\n",
    "            support = [preprocess_adj(adj)]\n",
    "        elif FLAGS.model == 'gcn_cheby':\n",
    "            support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "        # add NxN matrices along the num_supports dimension\n",
    "        sup = np.stack(support, axis=0)\n",
    "        # add num_supportsxNxN to support tensor\n",
    "        support_tensor[b,:,:,:,m] = sup\n",
    "\n",
    "# normalize all features\n",
    "features = preprocess_features(features)\n",
    "\n",
    "# Define placeholders\n",
    "F = features.shape[2]\n",
    "placeholders = {\n",
    "    'support': tf.placeholder(tf.float32, shape=(None,num_supports,N,N,M)), # ?xnum_supportsxNxNxM\n",
    "    'features': tf.placeholder(tf.float32, shape=(None,N,F)), # ?xNxF\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_arr.shape[1])), # ?,|labels|\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features.shape[2], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders, model):\n",
    "    t_test = time.time()\n",
    "    features = features[mask,:,:]\n",
    "    support = support[mask,:,:,:]\n",
    "    labels = labels[mask, :]\n",
    "    feed_dict = construct_feed_dict(features, support, labels, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(model.running_vars_initializer)\n",
    "\n",
    "# Train model\n",
    "t = time.time()\n",
    "cost_ls = []\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t_epoch = time.time()\n",
    "    \n",
    "    # instantiate all inputs\n",
    "    features_train = features[train_mask,:,:]\n",
    "    support = support_tensor[train_mask,:,:,:]\n",
    "    y_train = y_arr[train_mask, :]\n",
    "\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features_train, support, y_train, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Reset the counters\n",
    "    sess.run(model.running_vars_initializer)\n",
    "    \n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Reset the counters\n",
    "    sess.run(model.running_vars_initializer)\n",
    "    \n",
    "    # Validation\n",
    "    if save_validation:\n",
    "        cost, acc, duration = evaluate(features, support_tensor, y_arr, val_mask, placeholders, model)\n",
    "        cost_ls.append(cost)\n",
    "    \n",
    "    # Training\n",
    "    if save_test:\n",
    "        cost, acc, duration = evaluate(features, support_tensor, y_arr, test_mask, placeholders, model)\n",
    "        cost_ls.append(cost)\n",
    "        epoch_df.iloc[epoch, :] = [outs[1], outs[2], cost, acc, time.time() - t_epoch]\n",
    "    \n",
    "    # Print results\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]),\n",
    "          \"val/test_loss=\", \"{:.5f}\".format(cost), \"val/test_acc=\", \"{:.5f}\".format(acc),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        t = time.time()\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_ls[-1] > np.mean(cost_ls[max(-40, -1 * FLAGS.early_stopping):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished! Total Time: {} sec\".format(time.time() - ttot))\n",
    "\n",
    "# Reset the counters\n",
    "sess.run(model.running_vars_initializer)\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support_tensor, y_arr, test_mask, placeholders, model)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\n",
    "# save a text file whose name is the model_desc, graph_desc (from dataset), and has this info within it  \n",
    "if save_validation:\n",
    "    root = os.getcwd()\n",
    "    optimization = os.path.join(root, \"Optimization\")\n",
    "    validation_accuracy = acc\n",
    "    filename = \"{0:.5f}.{1}.{2}.outcome.txt\".format(validation_accuracy, model_desc, FLAGS.dataset)\n",
    "    filename = os.path.join(optimization, filename)\n",
    "    with open(filename, \"w\") as fh:\n",
    "        fh.write(\"{}\\n{}\\n{}\".format(validation_accuracy, model_desc, FLAGS.dataset))\n",
    "    \n",
    "# Saving results to a file\n",
    "if save_test:\n",
    "    # add column information\n",
    "    epoch_df.columns = [\"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\", \"time\"]\n",
    "    labels_df.columns = [\"Sequence\", \"Label\", \"Prediction\", \"Negative Class Logit\", \"Positive Class Logit\"]\n",
    "    \n",
    "    # get test values\n",
    "    features_test = features[test_mask,:,:]\n",
    "    support_test = support_tensor[test_mask,:,:,:]\n",
    "    labels_test = y_arr[test_mask, :]\n",
    "    \n",
    "    # add sequences\n",
    "    labels_df.iloc[:, 0] = [sequences[i] for i in range(len(test_mask)) if test_mask[i]]\n",
    "    \n",
    "    # add true labels\n",
    "    labels_df.iloc[:, 1] = [np.where(labels_test[i])[0] for i in range((sum(test_mask)))]\n",
    "    \n",
    "    # get logits in final layer and attention layer values\n",
    "    feed_dict = construct_feed_dict(features_test, support_test, labels_test, placeholders)\n",
    "    logits, predictions, attentions = sess.run([model.logits, model.predictions, model.attentions], feed_dict=feed_dict)\n",
    "    labels_df.iloc[:, 3:5] = logits\n",
    "    \n",
    "    # get predictions\n",
    "    labels_df.iloc[:, 2] = predictions\n",
    "    \n",
    "    # add attentions\n",
    "    att = np.zeros(shape = (attentions.shape[0] * attentions.shape[1], attentions.shape[2]))\n",
    "    for bat in range(attentions.shape[0]):\n",
    "        att[bat*attentions.shape[1]:(bat + 1)*attentions.shape[1],:] = attentions[bat,:,:]\n",
    "    attention_df = pd.DataFrame(att)\n",
    "    seq_test = [sequences[i] for i in range(len(test_mask)) if test_mask[i]]\n",
    "    bias_vals = []\n",
    "    batch_vals = []\n",
    "    s = []\n",
    "    for i in range(attentions.shape[0]): bias_vals += list(range(attentions.shape[1]))\n",
    "    for i in range(attentions.shape[0]): batch_vals += [i for j in range(attentions.shape[1])]\n",
    "    for i in range(attentions.shape[0]): s += [seq_test[i] for j in range(attentions.shape[1])]\n",
    "    attention_df[\"Bias\"] = bias_vals\n",
    "    attention_df[\"Batch\"] = batch_vals\n",
    "    attention_df[\"Sequence\"] = s\n",
    "    attention_df[\"N\"] = attentions.shape[2]\n",
    "    \n",
    "    # change indices for labels to their names\n",
    "    labels_df.iloc[:,1] = labels_df.iloc[:,1].map(lambda x: labelorder[x])\n",
    "    labels_df.iloc[:,2] = labels_df.iloc[:,2].map(lambda x: labelorder[x])\n",
    "    \n",
    "    # write to file\n",
    "    epoch_df.to_csv(\"../Results/{}.{}.epoch.csv\".format(model_desc, FLAGS.dataset), index = False)\n",
    "    labels_df.to_csv(\"../Results/{}.{}.predictions.csv\".format(model_desc, FLAGS.dataset), index = False)\n",
    "    attention_df.to_csv(\"../Results/{}.{}.attentions.csv\".format(model_desc, FLAGS.dataset), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Training| 1162, |Validation| 166, |Testing| 498\n",
      "Preprocessing adjacency lists\n",
      "2019-07-23 13:07:54.111629: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Optimization Finished! Total Time: 5.006397724151611 sec\n",
      "Test set results: cost= 1.43355 accuracy= 0.22490 time= 0.17001\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py -balanced_training True -learning_rate .01 -epochs 1 -early_stopping 500 -graph_conv_dimensions [20] -connected_dimensions [] -dropout 0 -attention_dim 10 -attention_bias 2 -model gcn -max_degree 1 -dataset selector_8_ang_ratio_0_params_all_onehot_distance -save_test True -save_validation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Training| 1162, |Validation| 166, |Testing| 498\n",
      "Calculating Chebyshev polynomials up to order 3...\n",
      "/mnt/c/Users/Owner/Documents/Code/Research_Scripts/PyRosetta/Graph-Convolutional-Neural-Network/GCNClassifier/utils.py:101: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
      "2019-07-23 13:27:17.995948: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Epoch: 0020 train_loss= 0.66243 train_acc= 0.72978 val/test_loss= 0.61199 val/test_acc= 0.77510 time= 94.78895\n",
      "Epoch: 0040 train_loss= 0.61249 train_acc= 0.72978 val/test_loss= 0.56390 val/test_acc= 0.77510 time= 93.18295\n",
      "Epoch: 0060 train_loss= 0.56423 train_acc= 0.72978 val/test_loss= 0.50677 val/test_acc= 0.77510 time= 94.77347\n",
      "Epoch: 0080 train_loss= 0.48193 train_acc= 0.76592 val/test_loss= 0.41816 val/test_acc= 0.79920 time= 91.42948\n",
      "Epoch: 0100 train_loss= 0.42977 train_acc= 0.81756 val/test_loss= 0.36748 val/test_acc= 0.82932 time= 92.93008\n",
      "Epoch: 0120 train_loss= 0.42640 train_acc= 0.82702 val/test_loss= 0.33204 val/test_acc= 0.87952 time= 94.41960\n",
      "Epoch: 0140 train_loss= 0.37557 train_acc= 0.86059 val/test_loss= 0.33456 val/test_acc= 0.86948 time= 92.41003\n",
      "Epoch: 0160 train_loss= 0.34885 train_acc= 0.87952 val/test_loss= 0.32695 val/test_acc= 0.88956 time= 93.00938\n",
      "Epoch: 0180 train_loss= 0.33960 train_acc= 0.88898 val/test_loss= 0.43767 val/test_acc= 0.86546 time= 93.10627\n",
      "Epoch: 0200 train_loss= 0.32699 train_acc= 0.90189 val/test_loss= 0.33993 val/test_acc= 0.88554 time= 92.98157\n",
      "Optimization Finished! Total Time: 987.5274896621704 sec\n",
      "Test set results: cost= 0.33993 accuracy= 0.88554 time= 0.90119\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py -learning_rate 0.01 -epochs 200 -early_stopping 600 -graph_conv_dimensions [20,20] -dropout 0 -attention_dim 10 -attention_bias 1 -model gcn_cheby -max_degree 3 -connected_dimensions [10,10] -dataset selector_8_ang_ratio_0_params_all_onehot_distance -save_test True -save_validation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 46.9 ms, total: 93.8 ms\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%time !python3 consolidate_results.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]]\n",
      "(26, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.]), array([[ 0.06820068],\n",
       "        [ 0.05824877],\n",
       "        [-0.15169597],\n",
       "        [ 0.03231936],\n",
       "        [ 0.0931789 ],\n",
       "        [ 0.00290181],\n",
       "        [ 0.06703742],\n",
       "        [ 0.36320261],\n",
       "        [-0.26347516],\n",
       "        [-0.05083581],\n",
       "        [-0.10620215],\n",
       "        [ 0.25611725],\n",
       "        [-0.12235471],\n",
       "        [ 0.20537812],\n",
       "        [ 0.30986563],\n",
       "        [ 0.24861462],\n",
       "        [-0.13556621],\n",
       "        [ 0.02169414],\n",
       "        [-0.12080429],\n",
       "        [ 0.45722021],\n",
       "        [-0.14662624],\n",
       "        [ 0.0022738 ],\n",
       "        [-0.13244341],\n",
       "        [ 0.02799337],\n",
       "        [ 0.37835751],\n",
       "        [ 0.18293973]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "laplacian = np.identity(26)\n",
    "print(type(laplacian))\n",
    "print(laplacian)\n",
    "print(laplacian.shape)\n",
    "eigsh(laplacian, 1, which='LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
